---
source_txt: fullstack_samples/mlflow-master
converted_utc: 2025-12-18T11:25:53Z
part: 365
parts_total: 991
---

# FULLSTACK CODE DATABASE SAMPLES mlflow-master

## Verbatim Content (Part 365 of 991)

````text
================================================================================
FULLSTACK SAMPLES CODE DATABASE (VERBATIM) - mlflow-master
================================================================================
Generated: December 18, 2025
Source: fullstack_samples/mlflow-master
================================================================================

NOTES:
- This output is verbatim because the source is user-owned.
- Large/binary files may be skipped by size/binary detection limits.

================================================================================

---[FILE: datasets_pb2.py]---
Location: mlflow-master/mlflow/protos/datasets_pb2.py

```python

import google.protobuf
from packaging.version import Version
if Version(google.protobuf.__version__).major >= 5:
  # -*- coding: utf-8 -*-
  # Generated by the protocol buffer compiler.  DO NOT EDIT!
  # source: datasets.proto
  # Protobuf Python Version: 5.26.0
  """Generated protocol buffer code."""
  from google.protobuf import descriptor as _descriptor
  from google.protobuf import descriptor_pool as _descriptor_pool
  from google.protobuf import symbol_database as _symbol_database
  from google.protobuf.internal import builder as _builder
  # @@protoc_insertion_point(imports)

  _sym_db = _symbol_database.Default()




  DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0e\x64\x61tasets.proto\x12\x0fmlflow.datasets\"\xdf\x01\n\x07\x44\x61taset\x12\x12\n\ndataset_id\x18\x01 \x01(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x0c\n\x04tags\x18\x03 \x01(\t\x12\x0e\n\x06schema\x18\x04 \x01(\t\x12\x0f\n\x07profile\x18\x05 \x01(\t\x12\x0e\n\x06\x64igest\x18\x06 \x01(\t\x12\x14\n\x0c\x63reated_time\x18\x07 \x01(\x03\x12\x18\n\x10last_update_time\x18\x08 \x01(\x03\x12\x12\n\ncreated_by\x18\t \x01(\t\x12\x17\n\x0flast_updated_by\x18\n \x01(\t\x12\x16\n\x0e\x65xperiment_ids\x18\x0b \x03(\t\"\xc9\x02\n\rDatasetRecord\x12\x19\n\x11\x64\x61taset_record_id\x18\x01 \x01(\t\x12\x12\n\ndataset_id\x18\x02 \x01(\t\x12\x0e\n\x06inputs\x18\x03 \x01(\t\x12\x14\n\x0c\x65xpectations\x18\x04 \x01(\t\x12\x0c\n\x04tags\x18\x05 \x01(\t\x12\x0e\n\x06source\x18\x06 \x01(\t\x12\x11\n\tsource_id\x18\x07 \x01(\t\x12\x44\n\x0bsource_type\x18\x08 \x01(\x0e\x32/.mlflow.datasets.DatasetRecordSource.SourceType\x12\x14\n\x0c\x63reated_time\x18\t \x01(\x03\x12\x18\n\x10last_update_time\x18\n \x01(\x03\x12\x12\n\ncreated_by\x18\x0b \x01(\t\x12\x17\n\x0flast_updated_by\x18\x0c \x01(\t\x12\x0f\n\x07outputs\x18\r \x01(\t\"\xc9\x01\n\x13\x44\x61tasetRecordSource\x12\x44\n\x0bsource_type\x18\x01 \x01(\x0e\x32/.mlflow.datasets.DatasetRecordSource.SourceType\x12\x13\n\x0bsource_data\x18\x02 \x01(\t\"W\n\nSourceType\x12\x1b\n\x17SOURCE_TYPE_UNSPECIFIED\x10\x00\x12\t\n\x05TRACE\x10\x01\x12\t\n\x05HUMAN\x10\x02\x12\x0c\n\x08\x44OCUMENT\x10\x03\x12\x08\n\x04\x43ODE\x10\x04\x42\x19\n\x14org.mlflow.api.proto\x90\x01\x01')

  _globals = globals()
  _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
  _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'datasets_pb2', _globals)
  if not _descriptor._USE_C_DESCRIPTORS:
    _globals['DESCRIPTOR']._loaded_options = None
    _globals['DESCRIPTOR']._serialized_options = b'\n\024org.mlflow.api.proto\220\001\001'
    _globals['_DATASET']._serialized_start=36
    _globals['_DATASET']._serialized_end=259
    _globals['_DATASETRECORD']._serialized_start=262
    _globals['_DATASETRECORD']._serialized_end=591
    _globals['_DATASETRECORDSOURCE']._serialized_start=594
    _globals['_DATASETRECORDSOURCE']._serialized_end=795
    _globals['_DATASETRECORDSOURCE_SOURCETYPE']._serialized_start=708
    _globals['_DATASETRECORDSOURCE_SOURCETYPE']._serialized_end=795
  # @@protoc_insertion_point(module_scope)

else:
  # -*- coding: utf-8 -*-
  # Generated by the protocol buffer compiler.  DO NOT EDIT!
  # source: datasets.proto
  """Generated protocol buffer code."""
  from google.protobuf import descriptor as _descriptor
  from google.protobuf import descriptor_pool as _descriptor_pool
  from google.protobuf import message as _message
  from google.protobuf import reflection as _reflection
  from google.protobuf import symbol_database as _symbol_database
  # @@protoc_insertion_point(imports)

  _sym_db = _symbol_database.Default()




  DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0e\x64\x61tasets.proto\x12\x0fmlflow.datasets\"\xdf\x01\n\x07\x44\x61taset\x12\x12\n\ndataset_id\x18\x01 \x01(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x0c\n\x04tags\x18\x03 \x01(\t\x12\x0e\n\x06schema\x18\x04 \x01(\t\x12\x0f\n\x07profile\x18\x05 \x01(\t\x12\x0e\n\x06\x64igest\x18\x06 \x01(\t\x12\x14\n\x0c\x63reated_time\x18\x07 \x01(\x03\x12\x18\n\x10last_update_time\x18\x08 \x01(\x03\x12\x12\n\ncreated_by\x18\t \x01(\t\x12\x17\n\x0flast_updated_by\x18\n \x01(\t\x12\x16\n\x0e\x65xperiment_ids\x18\x0b \x03(\t\"\xc9\x02\n\rDatasetRecord\x12\x19\n\x11\x64\x61taset_record_id\x18\x01 \x01(\t\x12\x12\n\ndataset_id\x18\x02 \x01(\t\x12\x0e\n\x06inputs\x18\x03 \x01(\t\x12\x14\n\x0c\x65xpectations\x18\x04 \x01(\t\x12\x0c\n\x04tags\x18\x05 \x01(\t\x12\x0e\n\x06source\x18\x06 \x01(\t\x12\x11\n\tsource_id\x18\x07 \x01(\t\x12\x44\n\x0bsource_type\x18\x08 \x01(\x0e\x32/.mlflow.datasets.DatasetRecordSource.SourceType\x12\x14\n\x0c\x63reated_time\x18\t \x01(\x03\x12\x18\n\x10last_update_time\x18\n \x01(\x03\x12\x12\n\ncreated_by\x18\x0b \x01(\t\x12\x17\n\x0flast_updated_by\x18\x0c \x01(\t\x12\x0f\n\x07outputs\x18\r \x01(\t\"\xc9\x01\n\x13\x44\x61tasetRecordSource\x12\x44\n\x0bsource_type\x18\x01 \x01(\x0e\x32/.mlflow.datasets.DatasetRecordSource.SourceType\x12\x13\n\x0bsource_data\x18\x02 \x01(\t\"W\n\nSourceType\x12\x1b\n\x17SOURCE_TYPE_UNSPECIFIED\x10\x00\x12\t\n\x05TRACE\x10\x01\x12\t\n\x05HUMAN\x10\x02\x12\x0c\n\x08\x44OCUMENT\x10\x03\x12\x08\n\x04\x43ODE\x10\x04\x42\x19\n\x14org.mlflow.api.proto\x90\x01\x01')



  _DATASET = DESCRIPTOR.message_types_by_name['Dataset']
  _DATASETRECORD = DESCRIPTOR.message_types_by_name['DatasetRecord']
  _DATASETRECORDSOURCE = DESCRIPTOR.message_types_by_name['DatasetRecordSource']
  _DATASETRECORDSOURCE_SOURCETYPE = _DATASETRECORDSOURCE.enum_types_by_name['SourceType']
  Dataset = _reflection.GeneratedProtocolMessageType('Dataset', (_message.Message,), {
    'DESCRIPTOR' : _DATASET,
    '__module__' : 'datasets_pb2'
    # @@protoc_insertion_point(class_scope:mlflow.datasets.Dataset)
    })
  _sym_db.RegisterMessage(Dataset)

  DatasetRecord = _reflection.GeneratedProtocolMessageType('DatasetRecord', (_message.Message,), {
    'DESCRIPTOR' : _DATASETRECORD,
    '__module__' : 'datasets_pb2'
    # @@protoc_insertion_point(class_scope:mlflow.datasets.DatasetRecord)
    })
  _sym_db.RegisterMessage(DatasetRecord)

  DatasetRecordSource = _reflection.GeneratedProtocolMessageType('DatasetRecordSource', (_message.Message,), {
    'DESCRIPTOR' : _DATASETRECORDSOURCE,
    '__module__' : 'datasets_pb2'
    # @@protoc_insertion_point(class_scope:mlflow.datasets.DatasetRecordSource)
    })
  _sym_db.RegisterMessage(DatasetRecordSource)

  if _descriptor._USE_C_DESCRIPTORS == False:

    DESCRIPTOR._options = None
    DESCRIPTOR._serialized_options = b'\n\024org.mlflow.api.proto\220\001\001'
    _DATASET._serialized_start=36
    _DATASET._serialized_end=259
    _DATASETRECORD._serialized_start=262
    _DATASETRECORD._serialized_end=591
    _DATASETRECORDSOURCE._serialized_start=594
    _DATASETRECORDSOURCE._serialized_end=795
    _DATASETRECORDSOURCE_SOURCETYPE._serialized_start=708
    _DATASETRECORDSOURCE_SOURCETYPE._serialized_end=795
  # @@protoc_insertion_point(module_scope)
```

--------------------------------------------------------------------------------

---[FILE: datasets_pb2.pyi]---
Location: mlflow-master/mlflow/protos/datasets_pb2.pyi

```text
from google.protobuf.internal import containers as _containers
from google.protobuf.internal import enum_type_wrapper as _enum_type_wrapper
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from typing import ClassVar as _ClassVar, Iterable as _Iterable, Optional as _Optional, Union as _Union

DESCRIPTOR: _descriptor.FileDescriptor

class Dataset(_message.Message):
    __slots__ = ("dataset_id", "name", "tags", "schema", "profile", "digest", "created_time", "last_update_time", "created_by", "last_updated_by", "experiment_ids")
    DATASET_ID_FIELD_NUMBER: _ClassVar[int]
    NAME_FIELD_NUMBER: _ClassVar[int]
    TAGS_FIELD_NUMBER: _ClassVar[int]
    SCHEMA_FIELD_NUMBER: _ClassVar[int]
    PROFILE_FIELD_NUMBER: _ClassVar[int]
    DIGEST_FIELD_NUMBER: _ClassVar[int]
    CREATED_TIME_FIELD_NUMBER: _ClassVar[int]
    LAST_UPDATE_TIME_FIELD_NUMBER: _ClassVar[int]
    CREATED_BY_FIELD_NUMBER: _ClassVar[int]
    LAST_UPDATED_BY_FIELD_NUMBER: _ClassVar[int]
    EXPERIMENT_IDS_FIELD_NUMBER: _ClassVar[int]
    dataset_id: str
    name: str
    tags: str
    schema: str
    profile: str
    digest: str
    created_time: int
    last_update_time: int
    created_by: str
    last_updated_by: str
    experiment_ids: _containers.RepeatedScalarFieldContainer[str]
    def __init__(self, dataset_id: _Optional[str] = ..., name: _Optional[str] = ..., tags: _Optional[str] = ..., schema: _Optional[str] = ..., profile: _Optional[str] = ..., digest: _Optional[str] = ..., created_time: _Optional[int] = ..., last_update_time: _Optional[int] = ..., created_by: _Optional[str] = ..., last_updated_by: _Optional[str] = ..., experiment_ids: _Optional[_Iterable[str]] = ...) -> None: ...

class DatasetRecord(_message.Message):
    __slots__ = ("dataset_record_id", "dataset_id", "inputs", "expectations", "tags", "source", "source_id", "source_type", "created_time", "last_update_time", "created_by", "last_updated_by", "outputs")
    DATASET_RECORD_ID_FIELD_NUMBER: _ClassVar[int]
    DATASET_ID_FIELD_NUMBER: _ClassVar[int]
    INPUTS_FIELD_NUMBER: _ClassVar[int]
    EXPECTATIONS_FIELD_NUMBER: _ClassVar[int]
    TAGS_FIELD_NUMBER: _ClassVar[int]
    SOURCE_FIELD_NUMBER: _ClassVar[int]
    SOURCE_ID_FIELD_NUMBER: _ClassVar[int]
    SOURCE_TYPE_FIELD_NUMBER: _ClassVar[int]
    CREATED_TIME_FIELD_NUMBER: _ClassVar[int]
    LAST_UPDATE_TIME_FIELD_NUMBER: _ClassVar[int]
    CREATED_BY_FIELD_NUMBER: _ClassVar[int]
    LAST_UPDATED_BY_FIELD_NUMBER: _ClassVar[int]
    OUTPUTS_FIELD_NUMBER: _ClassVar[int]
    dataset_record_id: str
    dataset_id: str
    inputs: str
    expectations: str
    tags: str
    source: str
    source_id: str
    source_type: DatasetRecordSource.SourceType
    created_time: int
    last_update_time: int
    created_by: str
    last_updated_by: str
    outputs: str
    def __init__(self, dataset_record_id: _Optional[str] = ..., dataset_id: _Optional[str] = ..., inputs: _Optional[str] = ..., expectations: _Optional[str] = ..., tags: _Optional[str] = ..., source: _Optional[str] = ..., source_id: _Optional[str] = ..., source_type: _Optional[_Union[DatasetRecordSource.SourceType, str]] = ..., created_time: _Optional[int] = ..., last_update_time: _Optional[int] = ..., created_by: _Optional[str] = ..., last_updated_by: _Optional[str] = ..., outputs: _Optional[str] = ...) -> None: ...

class DatasetRecordSource(_message.Message):
    __slots__ = ("source_type", "source_data")
    class SourceType(int, metaclass=_enum_type_wrapper.EnumTypeWrapper):
        __slots__ = ()
        SOURCE_TYPE_UNSPECIFIED: _ClassVar[DatasetRecordSource.SourceType]
        TRACE: _ClassVar[DatasetRecordSource.SourceType]
        HUMAN: _ClassVar[DatasetRecordSource.SourceType]
        DOCUMENT: _ClassVar[DatasetRecordSource.SourceType]
        CODE: _ClassVar[DatasetRecordSource.SourceType]
    SOURCE_TYPE_UNSPECIFIED: DatasetRecordSource.SourceType
    TRACE: DatasetRecordSource.SourceType
    HUMAN: DatasetRecordSource.SourceType
    DOCUMENT: DatasetRecordSource.SourceType
    CODE: DatasetRecordSource.SourceType
    SOURCE_TYPE_FIELD_NUMBER: _ClassVar[int]
    SOURCE_DATA_FIELD_NUMBER: _ClassVar[int]
    source_type: DatasetRecordSource.SourceType
    source_data: str
    def __init__(self, source_type: _Optional[_Union[DatasetRecordSource.SourceType, str]] = ..., source_data: _Optional[str] = ...) -> None: ...
```

--------------------------------------------------------------------------------

---[FILE: facet_feature_statistics.proto]---
Location: mlflow-master/mlflow/protos/facet_feature_statistics.proto

```proto
syntax = "proto2";

// Copyright 2017 Google Inc. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
// ==============================================================================

// Adapted from https://github.com/PAIR-code/facets/blob/4742b8b93c2dacf22fc8ace2cee42dd06382c48e/facets_overview/proto/feature_statistics.proto
// - Ported from proto3 to proto2 by adding "optional" modifiers to relevant fields
// - unsigned int64 types changed to int64 types

// Definitions for aggregated feature statistics for datasets.
package facetFeatureStatistics;

// A list of features statistics for different datasets. If you wish to compare
// different datasets using this list, then the DatasetFeatureStatistics
// entries should all contain the same list of features.
message DatasetFeatureStatisticsList {
  repeated DatasetFeatureStatistics datasets = 1;
}

// A Path is a structured way to identify features, as opposed to flat names.
message Path {
  // Any string is a valid step.
  // However, whenever possible have a step be [A-Za-z0-9_]+.
  repeated string step = 1;
}

// The feature statistics for a single dataset.
message DatasetFeatureStatistics {
  // The name of the dataset.
  optional string name = 1;
  // The number of examples in the dataset.
  optional int64 num_examples = 2;

  // Only valid if the weight feature was specified.
  // Treats a missing weighted feature as zero.
  optional double weighted_num_examples = 4;
  // The feature statistics for the dataset.
  repeated FeatureNameStatistics features = 3;
}

// The complete set of statistics for a given feature name for a dataset.
message FeatureNameStatistics {
  // The types supported by the feature statistics. When aggregating
  // tf.Examples, if the bytelist contains a string, it is recommended to encode
  // it here as STRING instead of BYTES in order to calculate string-specific
  // statistical measures.
  enum Type {
    INT = 0;
    FLOAT = 1;
    STRING = 2;
    BYTES = 3;
    STRUCT = 4;
  }

  // One can identify a feature either by the name (for simple fields), or by
  // a path (for structured fields). Note that:
  // name: "foo"
  // is equivalent to:
  // path: {step:"foo"}
  oneof field_id {
    // The feature name
    string name = 1;

    // The path of the feature.
    Path path = 8;
  }

  // The data type of the feature
  optional Type type = 2;

  // The statistics of the values of the feature.
  oneof stats {
    NumericStatistics num_stats = 3;
    StringStatistics string_stats = 4;
    BytesStatistics bytes_stats = 5;
    StructStatistics struct_stats = 7;
  }

  // Any custom statistics can be stored in this list.
  repeated CustomStatistic custom_stats = 6;
}

// Common weighted statistics for all feature types.
// If the weighted column is missing, then this counts as a weight of 1
// for that example.
message WeightedCommonStatistics {
  // Weighted number of examples not missing.
  optional double num_non_missing = 1;
  // Weighted number of examples missing.
  // Note that if the weighted column is zero, this does not count
  // as missing.
  optional double num_missing = 2;
  // average number of values, weighted by the number of examples.
  optional double avg_num_values = 3;
  // tot_num_values = avg_num_values * num_non_missing.
  // This is calculated directly, so should have less numerical error.
  optional double tot_num_values = 4;
}

// Stores the name and value of any custom statistic. The value can be a string,
// double, or histogram.
message CustomStatistic {
  optional string name = 1;
  oneof val {
    double num = 2;
    string str = 3;
    Histogram histogram = 4;
    RankHistogram rank_histogram = 5;
  }
  reserved 6;
}

// Statistics for a numeric feature in a dataset.
message NumericStatistics {
  optional CommonStatistics common_stats = 1;
  // The mean of the values
  optional double mean = 2;
  // The standard deviation of the values
  optional double std_dev = 3;
  // The number of values that equal 0
  optional int64 num_zeros = 4;
  // The minimum value
  optional double min = 5;
  // The median value
  optional double median = 6;
  // The maximum value
  optional double max = 7;
  // The histogram(s) of the feature values.
  repeated Histogram histograms = 8;

  // Weighted statistics for the feature, if the values have weights.
  optional WeightedNumericStatistics weighted_numeric_stats = 9;
}

// Statistics for a string feature in a dataset.
message StringStatistics {
  optional CommonStatistics common_stats = 1;
  // The number of unique values
  optional int64 unique = 2;

  message FreqAndValue {
    // DEPRECATED: The number of times the value occurs. Use 'frequency'
    // instead. If both fields are set, this field will be ignored.
    optional int64 deprecated_freq = 1 [deprecated = true];
    optional string value = 2;

    // The number of times the value occurs. Stored as a double to be able to
    // handle weighted features.
    optional double frequency = 3;
  }
  // A sorted list of the most-frequent values and their frequencies, with
  // the most-frequent being first.
  repeated FreqAndValue top_values = 3;

  // The average length of the values
  optional float avg_length = 4;

  // The rank histogram for the values of the feature.
  // The rank is used to measure of how commonly the value is found in the
  // dataset. The most common value would have a rank of 1, with the second-most
  // common value having a rank of 2, and so on.
  optional RankHistogram rank_histogram = 5;

  // Weighted statistics for the feature, if the values have weights.
  optional WeightedStringStatistics weighted_string_stats = 6;
}

// Statistics for a weighted numeric feature in a dataset.
message WeightedNumericStatistics {
  // The weighted mean of the values
  optional double mean = 1;
  // The weighted standard deviation of the values
  optional double std_dev = 2;
  // The weighted median of the values
  optional double median = 3;

  // The histogram(s) of the weighted feature values.
  repeated Histogram histograms = 4;
}

// Statistics for a weighted string feature in a dataset.
message WeightedStringStatistics {
  // A sorted list of the most-frequent values and their weighted frequencies,
  // with the most-frequent being first.
  repeated StringStatistics.FreqAndValue top_values = 1;

  // The rank histogram for the weighted values of the feature.
  optional RankHistogram rank_histogram = 2;
}

// Statistics for a bytes feature in a dataset.
message BytesStatistics {
  optional CommonStatistics common_stats = 1;
  // The number of unique values
  optional int64 unique = 2;

  // The average number of bytes in a value
  optional float avg_num_bytes = 3;
  // The minimum number of bytes in a value
  optional float min_num_bytes = 4;
  // The maximum number of bytes in a value
  optional float max_num_bytes = 5;
  // Reserved for field `max_num_bytes_int`
  reserved 6;
}

// Statistics for a struct feature in a dataset.
message StructStatistics {
  optional CommonStatistics common_stats = 1;
}

// Common statistics for all feature types
message CommonStatistics {
  // The number of examples with at least one value for this feature.
  optional int64 num_non_missing = 1;
  // The number of examples with no values for this feature.
  optional int64 num_missing = 2;
  // The minimum number of values in a single example for this feature.
  optional int64 min_num_values = 3;
  // The maximum number of values in a single example for this feature.
  optional int64 max_num_values = 4;
  // The average number of values in a single example for this feature.
  optional float avg_num_values = 5;
  // tot_num_values = avg_num_values * num_non_missing.
  // This is calculated directly, so should have less numerical error.
  optional int64 tot_num_values = 8;
  // The quantiles histogram for the number of values in this feature.
  optional Histogram num_values_histogram = 6;
  optional WeightedCommonStatistics weighted_common_stats = 7;
  // The histogram for the number of features in the feature list (only set if
  // this feature is a non-context feature from a tf.SequenceExample).
  // This is different from num_values_histogram, as num_values_histogram tracks
  // the count of all values for a feature in an example, whereas this tracks
  // the length of the feature list for this feature in an example (where each
  // feature list can contain multiple values).
  optional Histogram feature_list_length_histogram = 9;
  reserved 10, 11;
}

// The data used to create a histogram of a numeric feature for a dataset.
message Histogram {
  // Each bucket defines its low and high values along with its count. The
  // low and high values must be a real number or positive or negative
  // infinity. They cannot be NaN or undefined. Counts of those special values
  // can be found in the numNaN and numUndefined fields.
  message Bucket {
    // The low value of the bucket, inclusive.
    optional double low_value = 1;
    // The high value of the bucket, exclusive (unless the highValue is
    // positive infinity).
    optional double high_value = 2;

    // DEPRECATED: The number of items in the bucket. Use 'sampleCount' instead.
    // If both fields are set, this field will be ignored.
    optional int64 deprecated_count = 3 [deprecated = true];

    // The number of items in the bucket. Stored as a double to be able to
    // handle weighted histograms.
    optional double sample_count = 4;
  }

  // The number of NaN values in the dataset.
  optional int64 num_nan = 1;
  // The number of undefined values in the dataset.
  optional int64 num_undefined = 2;

  // A list of buckets in the histogram, sorted from lowest bucket to highest
  // bucket.
  repeated Bucket buckets = 3;

  // The type of the histogram. A standard histogram has equal-width buckets.
  // The quantiles type is used for when the histogram message is used to store
  // quantile information (by using equal-count buckets with variable widths).
  enum HistogramType {
    STANDARD = 0;
    QUANTILES = 1;
  }

  // The type of the histogram.
  optional HistogramType type = 4;

  // An optional descriptive name of the histogram, to be used for labeling.
  optional string name = 5;
}

// The data used to create a rank histogram of a non-numeric feature of a
// dataset. The rank of a value in a feature can be used as a measure of how
// commonly the value is found in the entire dataset. With bucket sizes of one,
// this becomes a distribution function of all feature values.
message RankHistogram {
  // Each bucket defines its start and end ranks along with its count.
  message Bucket {
    // The low rank of the bucket, inclusive.
    optional int64 low_rank = 1;
    // The high rank of the bucket, exclusive.
    optional int64 high_rank = 2;

    // DEPRECATED: The number of items in the bucket. Use 'sampleCount' instead.
    // If both fields are set, this field will be ignored.
    optional int64 deprecated_count = 3 [deprecated = true];

    // The label for the bucket. Can be used to list or summarize the values in
    // this rank bucket.
    optional string label = 4;

    // The number of items in the bucket. Stored as a double to be able to
    // handle weighted histograms.
    optional double sample_count = 5;
  }

  // A list of buckets in the histogram, sorted from lowest-ranked bucket to
  // highest-ranked bucket.
  repeated Bucket buckets = 1;

  // An optional descriptive name of the histogram, to be used for labeling.
  optional string name = 2;
}
```

--------------------------------------------------------------------------------

````
