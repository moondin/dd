---
source_txt: fullstack_samples/mlflow-master
converted_utc: 2025-12-18T11:25:53Z
part: 384
parts_total: 991
---

# FULLSTACK CODE DATABASE SAMPLES mlflow-master

## Verbatim Content (Part 384 of 991)

````text
================================================================================
FULLSTACK SAMPLES CODE DATABASE (VERBATIM) - mlflow-master
================================================================================
Generated: December 18, 2025
Source: fullstack_samples/mlflow-master
================================================================================

NOTES:
- This output is verbatim because the source is user-owned.
- Large/binary files may be skipped by size/binary detection limits.

================================================================================

---[FILE: scalapb_pb2.py]---
Location: mlflow-master/mlflow/protos/scalapb/scalapb_pb2.py

```python

import google.protobuf
from packaging.version import Version
if Version(google.protobuf.__version__).major >= 5:
  # -*- coding: utf-8 -*-
  # Generated by the protocol buffer compiler.  DO NOT EDIT!
  # source: scalapb/scalapb.proto
  # Protobuf Python Version: 5.26.0
  """Generated protocol buffer code."""
  from google.protobuf import descriptor as _descriptor
  from google.protobuf import descriptor_pool as _descriptor_pool
  from google.protobuf import symbol_database as _symbol_database
  from google.protobuf.internal import builder as _builder
  # @@protoc_insertion_point(imports)

  _sym_db = _symbol_database.Default()


  from google.protobuf import descriptor_pb2 as google_dot_protobuf_dot_descriptor__pb2


  DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x15scalapb/scalapb.proto\x12\x07scalapb\x1a google/protobuf/descriptor.proto\"L\n\x0eScalaPbOptions\x12\x14\n\x0cpackage_name\x18\x01 \x01(\t\x12\x14\n\x0c\x66lat_package\x18\x02 \x01(\x08\x12\x0e\n\x06import\x18\x03 \x03(\t\"!\n\x0eMessageOptions\x12\x0f\n\x07\x65xtends\x18\x01 \x03(\t\"\x1c\n\x0c\x46ieldOptions\x12\x0c\n\x04type\x18\x01 \x01(\t:G\n\x07options\x12\x1c.google.protobuf.FileOptions\x18\xfc\x07 \x01(\x0b\x32\x17.scalapb.ScalaPbOptions:J\n\x07message\x12\x1f.google.protobuf.MessageOptions\x18\xfc\x07 \x01(\x0b\x32\x17.scalapb.MessageOptions:D\n\x05\x66ield\x12\x1d.google.protobuf.FieldOptions\x18\xfc\x07 \x01(\x0b\x32\x15.scalapb.FieldOptionsB\x1e\n\x1corg.mlflow.scalapb_interface')

  _globals = globals()
  _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
  _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'scalapb.scalapb_pb2', _globals)
  if not _descriptor._USE_C_DESCRIPTORS:
    _globals['DESCRIPTOR']._loaded_options = None
    _globals['DESCRIPTOR']._serialized_options = b'\n\034org.mlflow.scalapb_interface'
    _globals['_SCALAPBOPTIONS']._serialized_start=68
    _globals['_SCALAPBOPTIONS']._serialized_end=144
    _globals['_MESSAGEOPTIONS']._serialized_start=146
    _globals['_MESSAGEOPTIONS']._serialized_end=179
    _globals['_FIELDOPTIONS']._serialized_start=181
    _globals['_FIELDOPTIONS']._serialized_end=209
  # @@protoc_insertion_point(module_scope)

else:
  # -*- coding: utf-8 -*-
  # Generated by the protocol buffer compiler.  DO NOT EDIT!
  # source: scalapb/scalapb.proto
  """Generated protocol buffer code."""
  from google.protobuf import descriptor as _descriptor
  from google.protobuf import descriptor_pool as _descriptor_pool
  from google.protobuf import message as _message
  from google.protobuf import reflection as _reflection
  from google.protobuf import symbol_database as _symbol_database
  # @@protoc_insertion_point(imports)

  _sym_db = _symbol_database.Default()


  from google.protobuf import descriptor_pb2 as google_dot_protobuf_dot_descriptor__pb2


  DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x15scalapb/scalapb.proto\x12\x07scalapb\x1a google/protobuf/descriptor.proto\"L\n\x0eScalaPbOptions\x12\x14\n\x0cpackage_name\x18\x01 \x01(\t\x12\x14\n\x0c\x66lat_package\x18\x02 \x01(\x08\x12\x0e\n\x06import\x18\x03 \x03(\t\"!\n\x0eMessageOptions\x12\x0f\n\x07\x65xtends\x18\x01 \x03(\t\"\x1c\n\x0c\x46ieldOptions\x12\x0c\n\x04type\x18\x01 \x01(\t:G\n\x07options\x12\x1c.google.protobuf.FileOptions\x18\xfc\x07 \x01(\x0b\x32\x17.scalapb.ScalaPbOptions:J\n\x07message\x12\x1f.google.protobuf.MessageOptions\x18\xfc\x07 \x01(\x0b\x32\x17.scalapb.MessageOptions:D\n\x05\x66ield\x12\x1d.google.protobuf.FieldOptions\x18\xfc\x07 \x01(\x0b\x32\x15.scalapb.FieldOptionsB\x1e\n\x1corg.mlflow.scalapb_interface')


  OPTIONS_FIELD_NUMBER = 1020
  options = DESCRIPTOR.extensions_by_name['options']
  MESSAGE_FIELD_NUMBER = 1020
  message = DESCRIPTOR.extensions_by_name['message']
  FIELD_FIELD_NUMBER = 1020
  field = DESCRIPTOR.extensions_by_name['field']

  _SCALAPBOPTIONS = DESCRIPTOR.message_types_by_name['ScalaPbOptions']
  _MESSAGEOPTIONS = DESCRIPTOR.message_types_by_name['MessageOptions']
  _FIELDOPTIONS = DESCRIPTOR.message_types_by_name['FieldOptions']
  ScalaPbOptions = _reflection.GeneratedProtocolMessageType('ScalaPbOptions', (_message.Message,), {
    'DESCRIPTOR' : _SCALAPBOPTIONS,
    '__module__' : 'scalapb.scalapb_pb2'
    # @@protoc_insertion_point(class_scope:scalapb.ScalaPbOptions)
    })
  _sym_db.RegisterMessage(ScalaPbOptions)

  MessageOptions = _reflection.GeneratedProtocolMessageType('MessageOptions', (_message.Message,), {
    'DESCRIPTOR' : _MESSAGEOPTIONS,
    '__module__' : 'scalapb.scalapb_pb2'
    # @@protoc_insertion_point(class_scope:scalapb.MessageOptions)
    })
  _sym_db.RegisterMessage(MessageOptions)

  FieldOptions = _reflection.GeneratedProtocolMessageType('FieldOptions', (_message.Message,), {
    'DESCRIPTOR' : _FIELDOPTIONS,
    '__module__' : 'scalapb.scalapb_pb2'
    # @@protoc_insertion_point(class_scope:scalapb.FieldOptions)
    })
  _sym_db.RegisterMessage(FieldOptions)

  if _descriptor._USE_C_DESCRIPTORS == False:
    google_dot_protobuf_dot_descriptor__pb2.FileOptions.RegisterExtension(options)
    google_dot_protobuf_dot_descriptor__pb2.MessageOptions.RegisterExtension(message)
    google_dot_protobuf_dot_descriptor__pb2.FieldOptions.RegisterExtension(field)

    DESCRIPTOR._options = None
    DESCRIPTOR._serialized_options = b'\n\034org.mlflow.scalapb_interface'
    _SCALAPBOPTIONS._serialized_start=68
    _SCALAPBOPTIONS._serialized_end=144
    _MESSAGEOPTIONS._serialized_start=146
    _MESSAGEOPTIONS._serialized_end=179
    _FIELDOPTIONS._serialized_start=181
    _FIELDOPTIONS._serialized_end=209
  # @@protoc_insertion_point(module_scope)
```

--------------------------------------------------------------------------------

---[FILE: scalapb_pb2.pyi]---
Location: mlflow-master/mlflow/protos/scalapb/scalapb_pb2.pyi

```text
from google.protobuf import descriptor_pb2 as _descriptor_pb2
from google.protobuf.internal import containers as _containers
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from typing import ClassVar as _ClassVar, Iterable as _Iterable, Optional as _Optional

DESCRIPTOR: _descriptor.FileDescriptor
OPTIONS_FIELD_NUMBER: _ClassVar[int]
options: _descriptor.FieldDescriptor
MESSAGE_FIELD_NUMBER: _ClassVar[int]
message: _descriptor.FieldDescriptor
FIELD_FIELD_NUMBER: _ClassVar[int]
field: _descriptor.FieldDescriptor

class ScalaPbOptions(_message.Message):
    __slots__ = ("package_name", "flat_package")
    PACKAGE_NAME_FIELD_NUMBER: _ClassVar[int]
    FLAT_PACKAGE_FIELD_NUMBER: _ClassVar[int]
    IMPORT_FIELD_NUMBER: _ClassVar[int]
    package_name: str
    flat_package: bool
    def __init__(self, package_name: _Optional[str] = ..., flat_package: bool = ..., **kwargs) -> None: ...

class MessageOptions(_message.Message):
    __slots__ = ("extends",)
    EXTENDS_FIELD_NUMBER: _ClassVar[int]
    extends: _containers.RepeatedScalarFieldContainer[str]
    def __init__(self, extends: _Optional[_Iterable[str]] = ...) -> None: ...

class FieldOptions(_message.Message):
    __slots__ = ("type",)
    TYPE_FIELD_NUMBER: _ClassVar[int]
    type: str
    def __init__(self, type: _Optional[str] = ...) -> None: ...
```

--------------------------------------------------------------------------------

---[FILE: autolog.py]---
Location: mlflow-master/mlflow/pydantic_ai/autolog.py

```python
import inspect
import logging
from dataclasses import asdict, is_dataclass
from typing import Any

import mlflow
from mlflow.entities import SpanType
from mlflow.entities.span import LiveSpan
from mlflow.tracing.constant import SpanAttributeKey, TokenUsageKey
from mlflow.utils.autologging_utils.config import AutoLoggingConfig

_logger = logging.getLogger(__name__)

_SAFE_PRIMITIVE_TYPES = (str, int, float, bool)


def _is_safe_for_serialization(value: Any) -> bool:
    if value is None:
        return False
    if isinstance(value, _SAFE_PRIMITIVE_TYPES):
        return True
    if isinstance(value, dict):
        return all(_is_safe_for_serialization(v) for v in value.values())
    if isinstance(value, (list, tuple)):
        return all(_is_safe_for_serialization(v) for v in value)
    if is_dataclass(value) and not isinstance(value, type):
        return True
    if isinstance(value, type):
        return True
    return False


def _safe_get_attribute(instance: Any, key: str) -> Any:
    try:
        value = getattr(instance, key, None)
        if value is None:
            return None
        if isinstance(value, type):
            return value.__name__
        if _is_safe_for_serialization(value):
            return value
        return None
    except Exception:
        return None


def _extract_safe_attributes(instance: Any) -> dict[str, Any]:
    """Extract all public attributes that are safe for serialization.

    Skips attributes starting with underscore to avoid capturing internal
    references (e.g., httpx clients) that can interfere with async cleanup.
    """
    attrs = {}
    for key in dir(instance):
        if key.startswith("_"):
            continue
        value = getattr(instance, key, None)
        # Skip methods/functions, but keep types (e.g., output_type=str)
        if callable(value) and not isinstance(value, type):
            continue
        safe_value = _safe_get_attribute(instance, key)
        if safe_value is not None:
            attrs[key] = safe_value
    return attrs


def _set_span_attributes(span: LiveSpan, instance):
    # 1) MCPServer attributes
    try:
        from pydantic_ai.mcp import MCPServer

        if isinstance(instance, MCPServer):
            mcp_attrs = _get_mcp_server_attributes(instance)
            span.set_attributes({k: v for k, v in mcp_attrs.items() if v is not None})
    except Exception as e:
        _logger.warning("Failed saving MCPServer attributes: %s", e)

    # 2) Agent attributes
    try:
        from pydantic_ai import Agent

        if isinstance(instance, Agent):
            agent_attrs = _get_agent_attributes(instance)
            span.set_attributes({k: v for k, v in agent_attrs.items() if v is not None})
    except Exception as e:
        _logger.warning("Failed saving Agent attributes: %s", e)

    # 3) InstrumentedModel attributes
    try:
        from pydantic_ai.models.instrumented import InstrumentedModel

        if isinstance(instance, InstrumentedModel):
            model_attrs = _get_model_attributes(instance)
            span.set_attributes({k: v for k, v in model_attrs.items() if v is not None})
    except Exception as e:
        _logger.warning("Failed saving InstrumentedModel attributes: %s", e)

    # 4) Tool attributes
    try:
        from pydantic_ai import Tool

        if isinstance(instance, Tool):
            tool_attrs = _get_tool_attributes(instance)
            span.set_attributes({k: v for k, v in tool_attrs.items() if v is not None})
    except Exception as e:
        _logger.warning("Failed saving Tool attributes: %s", e)


async def patched_async_class_call(original, self, *args, **kwargs):
    cfg = AutoLoggingConfig.init(flavor_name=mlflow.pydantic_ai.FLAVOR_NAME)
    if not cfg.log_traces:
        return await original(self, *args, **kwargs)

    fullname = f"{self.__class__.__name__}.{original.__name__}"
    span_type = _get_span_type(self)

    with mlflow.start_span(name=fullname, span_type=span_type) as span:
        inputs = _construct_full_inputs(original, self, *args, **kwargs)
        span.set_inputs(inputs)
        _set_span_attributes(span, self)

        result = await original(self, *args, **kwargs)
        outputs = _serialize_output(result)
        span.set_outputs(outputs)
        if usage_dict := _parse_usage(result):
            span.set_attribute(SpanAttributeKey.CHAT_USAGE, usage_dict)
        return result


def patched_class_call(original, self, *args, **kwargs):
    cfg = AutoLoggingConfig.init(flavor_name=mlflow.pydantic_ai.FLAVOR_NAME)
    if not cfg.log_traces:
        return original(self, *args, **kwargs)

    fullname = f"{self.__class__.__name__}.{original.__name__}"
    span_type = _get_span_type(self)
    with mlflow.start_span(name=fullname, span_type=span_type) as span:
        inputs = _construct_full_inputs(original, self, *args, **kwargs)
        span.set_inputs(inputs)
        _set_span_attributes(span, self)

        result = original(self, *args, **kwargs)
        outputs = _serialize_output(result)
        span.set_outputs(outputs)
        if usage_dict := _parse_usage(result):
            span.set_attribute(SpanAttributeKey.CHAT_USAGE, usage_dict)
        return result


def _get_span_type(instance) -> str:
    try:
        from pydantic_ai import Agent, Tool
        from pydantic_ai.mcp import MCPServer
        from pydantic_ai.models.instrumented import InstrumentedModel
    except ImportError:
        return SpanType.UNKNOWN

    if isinstance(instance, InstrumentedModel):
        return SpanType.LLM
    if isinstance(instance, Agent):
        return SpanType.AGENT
    if isinstance(instance, Tool):
        return SpanType.TOOL
    if isinstance(instance, MCPServer):
        return SpanType.TOOL

    try:
        from pydantic_ai._tool_manager import ToolManager

        if isinstance(instance, ToolManager):
            return SpanType.TOOL
    except ImportError:
        pass

    return SpanType.UNKNOWN


def _construct_full_inputs(func, *args, **kwargs) -> dict[str, Any]:
    try:
        sig = inspect.signature(func)
        bound = sig.bind_partial(*args, **kwargs).arguments
        bound.pop("self", None)
        bound.pop("deps", None)

        return {
            k: (v.__dict__ if hasattr(v, "__dict__") else v)
            for k, v in bound.items()
            if v is not None
        }
    except (ValueError, TypeError):
        return kwargs


def _serialize_output(result: Any) -> Any:
    if result is None:
        return None

    if hasattr(result, "new_messages") and callable(result.new_messages):
        try:
            new_messages = result.new_messages()
            serialized_messages = [asdict(msg) for msg in new_messages]
            serialized_result = asdict(result)
            serialized_result["_new_messages_serialized"] = serialized_messages
            return serialized_result
        except Exception as e:
            _logger.debug(f"Failed to serialize new_messages: {e}")

    return result.__dict__ if hasattr(result, "__dict__") else result


def _get_agent_attributes(instance):
    attrs = {SpanAttributeKey.MESSAGE_FORMAT: "pydantic_ai"}
    attrs.update(_extract_safe_attributes(instance))
    if hasattr(instance, "tools"):
        try:
            if tools_value := _parse_tools(instance.tools):
                attrs["tools"] = tools_value
        except Exception:
            pass
    return attrs


def _get_model_attributes(instance):
    attrs = {SpanAttributeKey.MESSAGE_FORMAT: "pydantic_ai"}
    attrs.update(_extract_safe_attributes(instance))
    return attrs


def _get_tool_attributes(instance):
    return _extract_safe_attributes(instance)


def _get_mcp_server_attributes(instance):
    attrs = _extract_safe_attributes(instance)
    if hasattr(instance, "tools"):
        try:
            if tools_value := _parse_tools(instance.tools):
                attrs["tools"] = tools_value
        except Exception:
            pass
    return attrs


def _parse_tools(tools):
    return [
        {"type": "function", "function": data}
        for tool in tools
        if (data := tool.model_dumps(exclude_none=True))
    ]


def _parse_usage(result: Any) -> dict[str, int] | None:
    try:
        if isinstance(result, tuple) and len(result) == 2:
            usage = result[1]
        else:
            usage = getattr(result, "usage", None)

        return {
            TokenUsageKey.INPUT_TOKENS: usage.request_tokens,
            TokenUsageKey.OUTPUT_TOKENS: usage.response_tokens,
            TokenUsageKey.TOTAL_TOKENS: usage.total_tokens,
        }
    except Exception as e:
        _logger.debug(f"Failed to parse token usage from output: {e}")
    return None
```

--------------------------------------------------------------------------------

---[FILE: __init__.py]---
Location: mlflow-master/mlflow/pydantic_ai/__init__.py

```python
import inspect
import logging

from mlflow.pydantic_ai.autolog import (
    patched_async_class_call,
    patched_class_call,
)
from mlflow.telemetry.events import AutologgingEvent
from mlflow.telemetry.track import _record_event
from mlflow.utils.autologging_utils import autologging_integration, safe_patch

FLAVOR_NAME = "pydantic_ai"
_logger = logging.getLogger(__name__)


@autologging_integration(FLAVOR_NAME)
def autolog(log_traces: bool = True, disable: bool = False, silent: bool = False):
    """
    Enable (or disable) autologging for Pydantic_AI.

    Args:
        log_traces: If True, capture spans for agent + model calls.
        disable:   If True, disable the autologging patches.
        silent:    If True, suppress MLflow warnings/info.
    """
    class_map = {
        "pydantic_ai.Agent": ["run", "run_sync"],
        "pydantic_ai.models.instrumented.InstrumentedModel": ["request"],
        "pydantic_ai._tool_manager.ToolManager": ["handle_call"],
        "pydantic_ai.mcp.MCPServer": ["call_tool", "list_tools"],
    }

    try:
        from pydantic_ai import Tool

        # Tool.run method is removed in recent versions
        if hasattr(Tool, "run"):
            class_map["pydantic_ai.Tool"] = ["run"]
    except ImportError:
        pass

    for cls_path, methods in class_map.items():
        module_name, class_name = cls_path.rsplit(".", 1)
        try:
            module = __import__(module_name, fromlist=[class_name])
            cls = getattr(module, class_name)
        except (ImportError, AttributeError) as e:
            _logger.error("Error importing %s: %s", cls_path, e)
            continue

        for method in methods:
            try:
                orig = getattr(cls, method)
                wrapper = (
                    patched_async_class_call
                    if inspect.iscoroutinefunction(orig)
                    else patched_class_call
                )
                safe_patch(
                    FLAVOR_NAME,
                    cls,
                    method,
                    wrapper,
                )
            except AttributeError as e:
                _logger.error("Error patching %s.%s: %s", cls_path, method, e)

    _record_event(
        AutologgingEvent, {"flavor": FLAVOR_NAME, "log_traces": log_traces, "disable": disable}
    )
```

--------------------------------------------------------------------------------

---[FILE: backend.py]---
Location: mlflow-master/mlflow/pyfunc/backend.py

```python
import ctypes
import json
import logging
import os
import pathlib
import shlex
import signal
import subprocess
import sys
import warnings
from pathlib import Path

from mlflow import pyfunc
from mlflow.exceptions import MlflowException
from mlflow.models import FlavorBackend, Model, docker_utils
from mlflow.models.docker_utils import PYTHON_SLIM_BASE_IMAGE, UBUNTU_BASE_IMAGE
from mlflow.pyfunc import (
    ENV,
    _extract_conda_env,
    _mlflow_pyfunc_backend_predict,
    mlserver,
    scoring_server,
)
from mlflow.tracking.artifact_utils import _download_artifact_from_uri
from mlflow.utils import env_manager as em
from mlflow.utils.conda import get_conda_bin_executable, get_or_create_conda_env
from mlflow.utils.environment import Environment, _get_pip_install_mlflow, _PythonEnv
from mlflow.utils.file_utils import (
    TempDir,
    get_or_create_nfs_tmp_dir,
    get_or_create_tmp_dir,
    path_to_local_file_uri,
)
from mlflow.utils.model_utils import _get_all_flavor_configurations
from mlflow.utils.nfs_on_spark import get_nfs_cache_root_dir
from mlflow.utils.os import is_windows
from mlflow.utils.process import ShellCommandException, cache_return_value_per_process
from mlflow.utils.virtualenv import _get_or_create_virtualenv
from mlflow.version import VERSION

_logger = logging.getLogger(__name__)

_STDIN_SERVER_SCRIPT = Path(__file__).parent.joinpath("stdin_server.py")

# Flavors that require Java to be installed in the environment
JAVA_FLAVORS = {"johnsnowlabs", "h2o", "spark"}

# Some flavor requires additional packages to be installed in the environment
FLAVOR_SPECIFIC_APT_PACKAGES = {
    "lightgbm": ["libgomp1"],
    "paddle": ["libgomp1"],
}

# Directory to store loaded model inside the Docker context directory
_MODEL_DIR_NAME = "model_dir"
LOCAL_ENV_MANAGER_ERROR_MESSAGE = "We cannot use 'LOCAL' environment manager "
"for your model configuration. Please specify a virtualenv or conda environment "
"manager instead with `--env-manager` argument."


def _set_mlflow_config_env(command_env, model_config):
    if model_config:
        command_env[scoring_server.SERVING_MODEL_CONFIG] = json.dumps(model_config)
    return command_env


class PyFuncBackend(FlavorBackend):
    """
    Flavor backend implementation for the generic python models.
    """

    def __init__(
        self,
        config,
        env_manager,
        workers=1,
        install_mlflow=False,
        create_env_root_dir=False,
        env_root_dir=None,
        **kwargs,
    ):
        """
        Args:
            env_manager: Environment manager to use for preparing the environment. If None,
                MLflow will automatically pick the env manager based on the model's flavor
                configuration for generate_dockerfile. It can't be None for other methods.
            env_root_dir: Root path for conda env. If None, use Conda's default environments
                directory. Note if this is set, conda package cache path becomes
                "{env_root_dir}/conda_cache_pkgs" instead of the global package cache
                path, and pip package cache path becomes
                "{env_root_dir}/pip_cache_pkgs" instead of the global package cache
                path.
        """
        super().__init__(config=config, **kwargs)
        self._nworkers = workers or 1
        if env_manager == em.CONDA and ENV not in config:
            warnings.warn(
                "Conda environment is not specified in config `env`. Using local environment."
            )
            env_manager = em.LOCAL
        self._env_manager = env_manager
        self._install_mlflow = install_mlflow
        self._env_id = os.environ.get("MLFLOW_HOME", VERSION) if install_mlflow else None
        self._create_env_root_dir = create_env_root_dir
        self._env_root_dir = env_root_dir
        self._environment = None

    def prepare_env(
        self, model_uri, capture_output=False, pip_requirements_override=None, extra_envs=None
    ):
        if self._environment is not None:
            return self._environment

        @cache_return_value_per_process
        def _get_or_create_env_root_dir(should_use_nfs):
            if should_use_nfs:
                root_tmp_dir = get_or_create_nfs_tmp_dir()
            else:
                root_tmp_dir = get_or_create_tmp_dir()

            envs_root_dir = os.path.join(root_tmp_dir, "envs")
            os.makedirs(envs_root_dir, exist_ok=True)
            return envs_root_dir

        local_path = _download_artifact_from_uri(model_uri)
        if self._create_env_root_dir:
            if self._env_root_dir is not None:
                raise Exception("env_root_dir can not be set when create_env_root_dir=True")
            nfs_root_dir = get_nfs_cache_root_dir()
            env_root_dir = _get_or_create_env_root_dir(nfs_root_dir is not None)
        else:
            env_root_dir = self._env_root_dir

        if self._env_manager in {em.VIRTUALENV, em.UV}:
            activate_cmd = _get_or_create_virtualenv(
                local_path,
                self._env_id,
                env_root_dir=env_root_dir,
                capture_output=capture_output,
                pip_requirements_override=pip_requirements_override,
                env_manager=self._env_manager,
                extra_envs=extra_envs,
            )
            self._environment = Environment(activate_cmd, extra_env=extra_envs)
        elif self._env_manager == em.CONDA:
            conda_env_path = os.path.join(local_path, _extract_conda_env(self._config[ENV]))
            self._environment = get_or_create_conda_env(
                conda_env_path,
                env_id=self._env_id,
                capture_output=capture_output,
                env_root_dir=env_root_dir,
                pip_requirements_override=pip_requirements_override,
                extra_envs=extra_envs,
            )

        elif self._env_manager == em.LOCAL:
            raise Exception("Prepare env should not be called with local env manager!")
        else:
            raise Exception(f"Unexpected env manager value '{self._env_manager}'")

        if self._install_mlflow:
            self._environment.execute(_get_pip_install_mlflow())
        else:
            self._environment.execute('python -c ""')

        return self._environment

    def predict(
        self,
        model_uri,
        input_path,
        output_path,
        content_type,
        pip_requirements_override=None,
        extra_envs=None,
    ):
        """
        Generate predictions using generic python model saved with MLflow. The expected format of
        the input JSON is the MLflow scoring format.
        Return the prediction results as a JSON.
        """
        local_path = _download_artifact_from_uri(model_uri)
        # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure
        # platform compatibility.
        local_uri = path_to_local_file_uri(local_path)

        if self._env_manager != em.LOCAL:
            predict_cmd = [
                "python",
                _mlflow_pyfunc_backend_predict.__file__,
                "--model-uri",
                str(local_uri),
                "--content-type",
                shlex.quote(str(content_type)),
            ]
            if input_path:
                predict_cmd += ["--input-path", shlex.quote(str(input_path))]
            if output_path:
                predict_cmd += ["--output-path", shlex.quote(str(output_path))]

            if pip_requirements_override and self._env_manager == em.CONDA:
                # Conda use = instead of == for version pinning
                pip_requirements_override = [
                    pip_req.replace("==", "=") for pip_req in pip_requirements_override
                ]

            environment = self.prepare_env(
                local_path,
                pip_requirements_override=pip_requirements_override,
                extra_envs=extra_envs,
            )

            try:
                environment.execute(" ".join(predict_cmd))
            except ShellCommandException as e:
                raise MlflowException(
                    f"{e}\n\nAn exception occurred while running model prediction within a "
                    f"{self._env_manager} environment. You can find the error message "
                    f"from the prediction subprocess by scrolling above."
                ) from None
        else:
            if pip_requirements_override:
                raise MlflowException(
                    "`pip_requirements_override` is not supported for local env manager."
                    "Please use conda or virtualenv instead."
                )
            scoring_server._predict(local_uri, input_path, output_path, content_type)

    def serve(
        self,
        model_uri,
        port,
        host,
        timeout,
        enable_mlserver,
        synchronous=True,
        stdout=None,
        stderr=None,
        model_config=None,
    ):
        """
        Serve pyfunc model locally.
        """
        local_path = _download_artifact_from_uri(model_uri)

        server_implementation = mlserver if enable_mlserver else scoring_server
        command, command_env = server_implementation.get_cmd(
            local_path, port, host, timeout, self._nworkers
        )
        _set_mlflow_config_env(command_env, model_config)

        if sys.platform.startswith("linux"):

            def setup_sigterm_on_parent_death():
                """
                Uses prctl to automatically send SIGTERM to the command process when its parent is
                dead.

                This handles the case when the parent is a PySpark worker process.
                If a user cancels the PySpark job, the worker process gets killed, regardless of
                PySpark daemon and worker reuse settings.
                We use prctl to ensure the command process receives SIGTERM after spark job
                cancellation.
                The command process itself should handle SIGTERM properly.
                This is a no-op on macOS because prctl is not supported.

                Note:
                When a pyspark job canceled, the UDF python process are killed by signal "SIGKILL",
                This case neither "atexit" nor signal handler can capture SIGKILL signal.
                prctl is the only way to capture SIGKILL signal.
                """
                try:
                    libc = ctypes.CDLL("libc.so.6")
                    # Set the parent process death signal of the command process to SIGTERM.
                    libc.prctl(1, signal.SIGTERM)  # PR_SET_PDEATHSIG, see prctl.h
                except OSError as e:
                    # TODO: find approach for supporting MacOS/Windows system which does
                    #  not support prctl.
                    warnings.warn(f"Setup libc.prctl PR_SET_PDEATHSIG failed, error {e!r}.")

        else:
            setup_sigterm_on_parent_death = None

        if not is_windows():
            # Add "exec" before the starting scoring server command, so that the scoring server
            # process replaces the bash process, otherwise the scoring server process is created
            # as a child process of the bash process.
            # Note we in `mlflow.pyfunc.spark_udf`, use prctl PR_SET_PDEATHSIG to ensure scoring
            # server process being killed when UDF process exit. The PR_SET_PDEATHSIG can only
            # send signal to the bash process, if the scoring server process is created as a
            # child process of the bash process, then it cannot receive the signal sent by prctl.
            # TODO: For Windows, there's no equivalent things of Unix shell's exec. Windows also
            #  does not support prctl. We need to find an approach to address it.
            command = "exec " + command

        if self._env_manager != em.LOCAL:
            return self.prepare_env(local_path).execute(
                command,
                command_env,
                stdout=stdout,
                stderr=stderr,
                preexec_fn=setup_sigterm_on_parent_death,
                synchronous=synchronous,
            )
        else:
            _logger.info("=== Running command '%s'", command)

            if not is_windows():
                command = ["bash", "-c", command]

            child_proc = subprocess.Popen(
                command,
                env=command_env,
                preexec_fn=setup_sigterm_on_parent_death,
                stdout=stdout,
                stderr=stderr,
            )

            if synchronous:
                rc = child_proc.wait()
                if rc != 0:
                    raise Exception(
                        f"Command '{command}' returned non zero return code. Return code = {rc}"
                    )
                return 0
            else:
                return child_proc

    def serve_stdin(
        self,
        model_uri,
        stdout=None,
        stderr=None,
        model_config=None,
    ):
        local_path = _download_artifact_from_uri(model_uri)

        command_env = os.environ.copy()
        _set_mlflow_config_env(command_env, model_config)

        return self.prepare_env(local_path).execute(
            command=f"python {_STDIN_SERVER_SCRIPT} --model-uri {local_path}",
            command_env=command_env,
            stdin=subprocess.PIPE,
            stdout=stdout,
            stderr=stderr,
            synchronous=False,
        )

    def can_score_model(self):
        if self._env_manager == em.LOCAL:
            # noconda => already in python and dependencies are assumed to be installed.
            return True
        conda_path = get_conda_bin_executable("conda")
        try:
            p = subprocess.Popen(
                [conda_path, "--version"], stdout=subprocess.PIPE, stderr=subprocess.PIPE
            )
            _, _ = p.communicate()
            return p.wait() == 0
        except FileNotFoundError:
            # Can not find conda
            return False

    def build_image(
        self,
        model_uri,
        image_name,
        install_java=False,
        install_mlflow=False,
        mlflow_home=None,
        enable_mlserver=False,
        base_image=None,
    ):
        with TempDir() as tmp:
            cwd = tmp.path()
            self.generate_dockerfile(
                model_uri=model_uri,
                output_dir=cwd,
                install_java=install_java,
                install_mlflow=install_mlflow,
                mlflow_home=mlflow_home,
                enable_mlserver=enable_mlserver,
                base_image=base_image,
            )

            _logger.info("Building docker image with name %s", image_name)
            docker_utils.build_image_from_context(context_dir=cwd, image_name=image_name)

    def generate_dockerfile(
        self,
        model_uri,
        output_dir,
        install_java=False,
        install_mlflow=False,
        mlflow_home=None,
        enable_mlserver=False,
        base_image=None,
    ):
        os.makedirs(output_dir, exist_ok=True)
        _logger.debug("Created all folders in path", extra={"output_directory": output_dir})

        if model_uri:
            model_cwd = os.path.join(output_dir, _MODEL_DIR_NAME)
            pathlib.Path(model_cwd).mkdir(parents=True, exist_ok=True)
            model_path = _download_artifact_from_uri(model_uri, output_path=model_cwd)
            base_image = base_image or self._get_base_image(model_path, install_java)
            env_manager = self._env_manager or em.LOCAL

            if base_image.startswith("python"):
                # we can directly use local env for python image
                if env_manager in [em.CONDA, em.VIRTUALENV]:
                    # we can directly use ubuntu image for conda and virtualenv
                    base_image = UBUNTU_BASE_IMAGE
            elif base_image == UBUNTU_BASE_IMAGE:
                env_manager = self._env_manager or em.VIRTUALENV
                # installing python on ubuntu image is problematic and not recommended officially
                # , so we recommend using conda or virtualenv instead on ubuntu image
                if env_manager == em.LOCAL:
                    raise MlflowException.invalid_parameter_value(LOCAL_ENV_MANAGER_ERROR_MESSAGE)

            copy_src = os.path.relpath(model_path, start=output_dir)
            model_install_steps = self._model_installation_steps(
                copy_src,
                model_path,
                env_manager,
                install_mlflow,
                enable_mlserver,
            )
            entrypoint = f"from mlflow.models import container as C; C._serve('{env_manager}')"

        # if no model_uri specified, user must use virtualenv or conda env based on ubuntu image
        else:
            base_image = base_image or UBUNTU_BASE_IMAGE
            env_manager = self._env_manager or em.VIRTUALENV
            if env_manager == em.LOCAL:
                raise MlflowException.invalid_parameter_value(LOCAL_ENV_MANAGER_ERROR_MESSAGE)

            model_install_steps = ""
            # If model_uri is not specified, dependencies are installed at runtime
            entrypoint = (
                self._get_install_pyfunc_deps_cmd(env_manager, install_mlflow, enable_mlserver)
                + f" C._serve('{env_manager}')"
            )

        dockerfile_text = docker_utils.generate_dockerfile(
            output_dir=output_dir,
            base_image=base_image,
            model_install_steps=model_install_steps,
            entrypoint=entrypoint,
            env_manager=env_manager,
            mlflow_home=mlflow_home,
            enable_mlserver=enable_mlserver,
            # always disable env creation at runtime for pyfunc
            disable_env_creation_at_runtime=True,
        )
        _logger.debug("generated dockerfile at {output_dir}", extra={"dockerfile": dockerfile_text})

    def _get_base_image(self, model_path: str, install_java: bool) -> str:
        """
        Determine the base image to use for the Dockerfile.

        We use Python slim base image when all the following conditions are met:
          1. Model URI is specified by the user
          2. Model flavor does not require Java
          3. Python version is specified in the model

        Returns:
            Either the Ubuntu base image or the Python slim base image.
        """
        # Check if the model requires Java
        if not install_java:
            flavors = _get_all_flavor_configurations(model_path).keys()
            if java_flavors := JAVA_FLAVORS & flavors:
                _logger.info(f"Detected java flavors {java_flavors}, installing Java in the image")
                install_java = True

        # Use ubuntu base image if Java is required
        if install_java:
            return UBUNTU_BASE_IMAGE

        # Get Python version from MLmodel
        try:
            env_conf = Model.load(model_path).flavors[pyfunc.FLAVOR_NAME][pyfunc.ENV][em.VIRTUALENV]
            python_env_config_path = os.path.join(model_path, env_conf)

            python_env = _PythonEnv.from_yaml(python_env_config_path)
            return PYTHON_SLIM_BASE_IMAGE.format(version=python_env.python)
        except Exception as e:
            _logger.warning(
                f"Failed to determine Python version from {model_path}. "
                f"Defaulting to {UBUNTU_BASE_IMAGE}. Error: {e}"
            )
            return UBUNTU_BASE_IMAGE

    def _model_installation_steps(
        self, copy_src, model_path, env_manager, install_mlflow, enable_mlserver
    ):
        # Copy model to image if model_uri is specified
        steps = (
            "# Copy model to image and install dependencies\n"
            f"COPY {copy_src} /opt/ml/model\nRUN python -c "
        )
        steps += (
            f'"{self._get_install_pyfunc_deps_cmd(env_manager, install_mlflow, enable_mlserver)}"'
        )

        # Install flavor-specific dependencies if needed
        flavors = _get_all_flavor_configurations(model_path).keys()
        for flavor in flavors:
            if flavor in FLAVOR_SPECIFIC_APT_PACKAGES:
                packages = " ".join(FLAVOR_SPECIFIC_APT_PACKAGES[flavor])
                steps += f"\nRUN apt-get install -y --no-install-recommends {packages}"

        return steps

    def _get_install_pyfunc_deps_cmd(
        self, env_manager: str, install_mlflow: bool, enable_mlserver: bool
    ):
        return (
            "from mlflow.models import container as C; "
            f"C._install_pyfunc_deps('/opt/ml/model', install_mlflow={install_mlflow}, "
            f"enable_mlserver={enable_mlserver}, env_manager='{env_manager}');"
        )
```

--------------------------------------------------------------------------------

````
