---
source_txt: fullstack_samples/mlflow-master
converted_utc: 2025-12-18T11:25:52Z
part: 19
parts_total: 991
---

# FULLSTACK CODE DATABASE SAMPLES mlflow-master

## Verbatim Content (Part 19 of 991)

````text
================================================================================
FULLSTACK SAMPLES CODE DATABASE (VERBATIM) - mlflow-master
================================================================================
Generated: December 18, 2025
Source: fullstack_samples/mlflow-master
================================================================================

NOTES:
- This output is verbatim because the source is user-owned.
- Large/binary files may be skipped by size/binary detection limits.

================================================================================

---[FILE: pyproject.py]---
Location: mlflow-master/dev/pyproject.py
Signals: Flask, Pydantic, SQLAlchemy

```python
from __future__ import annotations

import re
import subprocess
import sys
from collections import Counter
from enum import Enum
from pathlib import Path
from typing import Any

import toml
import yaml
from packaging.version import Version
from pydantic import BaseModel, Field, RootModel


class PackageType(Enum):
    SKINNY = "skinny"
    RELEASE = "release"
    DEV = "dev"
    TRACING = "tracing"

    def description(self) -> str:
        WARNING = "# Auto-generated by dev/pyproject.py. Do not edit manually."

        if self is PackageType.TRACING:
            return f"""{WARNING}
# This file defines the package metadata of `mlflow-tracing`.
"""

        if self is PackageType.SKINNY:
            return f"""{WARNING}
# This file defines the package metadata of `mlflow-skinny`.
"""
        if self is PackageType.RELEASE:
            return f"""{WARNING}
# This file defines the package metadata of `mlflow`. `mlflow-skinny` and `mlflow-tracing`
# are included in the requirements to prevent a version mismatch between `mlflow` and those
# child packages. This file will replace `pyproject.toml` when releasing a new version.
"""
        if self is PackageType.DEV:
            return f"""{WARNING}
# This file defines the package metadata of `mlflow` **during development**. To install `mlflow`
# from the source code, `mlflow-skinny` and `mlflow-tracing` are NOT included in the requirements.
# This file will be replaced by `pyproject.release.toml` when releasing a new version.
"""
        raise ValueError(f"Unreachable: {self}")


SEPARATOR = """
# Package metadata: can't be updated manually, use dev/pyproject.py
# -----------------------------------------------------------------
# Dev tool settings: can be updated manually

"""

SKINNY_README = """
<!--  Autogenerated by dev/pyproject.py. Do not edit manually.  -->

ðŸ“£ This is the `mlflow-skinny` package, a lightweight MLflow package without SQL storage, server, UI, or data science dependencies.
Additional dependencies can be installed to leverage the full feature set of MLflow. For example:

- To use the `mlflow.sklearn` component of MLflow Models, install `scikit-learn`, `numpy` and `pandas`.
- To use SQL-based metadata storage, install `sqlalchemy`, `alembic`, and `sqlparse`.
- To use serving-based features, install `flask` and `pandas`.

---

<br>
<br>

"""  # noqa: E501

# Tracing SDK should only include the minimum set of MLflow modules
# to minimize the size of the package.
TRACING_INCLUDE_FILES = [
    "mlflow",
    # Flavors that we support auto tracing
    "mlflow.agno*",
    "mlflow.anthropic*",
    "mlflow.autogen*",
    "mlflow.bedrock*",
    "mlflow.crewai*",
    "mlflow.dspy*",
    "mlflow.gemini*",
    "mlflow.groq*",
    "mlflow.langchain*",
    "mlflow.litellm*",
    "mlflow.llama_index*",
    "mlflow.mistral*",
    "mlflow.openai*",
    "mlflow.strands*",
    "mlflow.haystack*",
    # Other necessary modules
    "mlflow.azure*",
    "mlflow.entities*",
    "mlflow.environment_variables",
    "mlflow.exceptions",
    "mlflow.legacy_databricks_cli*",
    "mlflow.prompt*",
    "mlflow.protos*",
    "mlflow.pydantic_ai*",
    "mlflow.smolagents*",
    "mlflow.store*",
    "mlflow.telemetry*",
    "mlflow.tracing*",
    "mlflow.tracking*",
    "mlflow.types*",
    "mlflow.utils*",
    "mlflow.version",
]
TRACING_EXCLUDE_FILES = [
    # Large proto files that are not needed in the package
    "mlflow/protos/databricks_artifacts_pb2.py",
    "mlflow/protos/databricks_filesystem_service_pb2.py",
    "mlflow/protos/databricks_uc_registry_messages_pb2.py",
    "mlflow/protos/databricks_uc_registry_service_pb2.py",
    "mlflow/protos/model_registry_pb2.py",
    "mlflow/protos/unity_catalog_oss_messages_pb2.py",
    "mlflow/protos/unity_catalog_oss_service_pb2.py",
    # Test files
    "tests",
    "tests.*",
]


def find_duplicates(seq):
    counted = Counter(seq)
    return [item for item, count in counted.items() if count > 1]


def write_file_if_changed(file_path: Path, new_content: str) -> None:
    if file_path.exists():
        existing_content = file_path.read_text()
        if existing_content == new_content:
            print(f"No changes in {file_path}, skipping write.")
            return

    print(f"Writing changes to {file_path}.")
    file_path.write_text(new_content)


def format_content_with_taplo(content: str) -> str:
    return (
        subprocess.check_output(
            ["bin/taplo", "fmt", "-"],
            input=content,
            text=True,
        ).strip()
        + "\n"
    )


def write_toml_file_if_changed(
    file_path: Path, description: str, toml_data: dict[str, Any]
) -> None:
    """
    Write a TOML file with description only if content has changed.
    Formats content with taplo before comparison.
    """
    new_content = description + "\n" + toml.dumps(toml_data)
    formatted_content = format_content_with_taplo(new_content)
    write_file_if_changed(file_path, formatted_content)


class PackageRequirement(BaseModel):
    pip_release: str = Field(..., description="The pip package name")
    max_major_version: int = Field(..., description="Maximum major version allowed")
    minimum: str | None = Field(None, description="Minimum version required")
    unsupported: list[str] | None = Field(None, description="List of unsupported versions")
    markers: str | None = Field(
        None, description="Environment markers for conditional installation"
    )
    extras: list[str] | None = Field(None, description="Package extras to install")
    freeze: bool | None = Field(None, description="Whether to freeze this package version")


RequirementsYaml = RootModel[dict[str, PackageRequirement]]


def generate_requirements_from_yaml(requirements_yaml: RequirementsYaml) -> list[str]:
    """Generate pip requirement strings from validated YAML specification."""
    requirement_strs: list[str] = []
    for package_entry in requirements_yaml.root.values():
        pip_release = package_entry.pip_release
        version_specs: list[str] = []

        extras = f"[{','.join(package_entry.extras)}]" if package_entry.extras else ""

        max_major_version = package_entry.max_major_version
        version_specs.append(f"<{max_major_version + 1}")

        if package_entry.minimum:
            version_specs.append(f">={package_entry.minimum}")

        if package_entry.unsupported:
            version_specs.extend(f"!={version}" for version in package_entry.unsupported)

        markers = f"; {package_entry.markers}" if package_entry.markers else ""

        requirement_str = f"{pip_release}{extras}{','.join(version_specs)}{markers}"
        requirement_strs.append(requirement_str)

    requirement_strs.sort()
    return requirement_strs


def read_requirements_yaml(yaml_path: Path) -> list[str]:
    """Read and parse a YAML requirements file into pip requirement strings."""
    with yaml_path.open() as f:
        requirements_data = yaml.safe_load(f)

    return generate_requirements_from_yaml(RequirementsYaml(requirements_data))


def read_package_versions_yml():
    with open("mlflow/ml-package-versions.yml") as f:
        return yaml.safe_load(f)


def build(package_type: PackageType) -> None:
    requirements_dir = Path("requirements")
    tracing_requirements = read_requirements_yaml(requirements_dir / "tracing-requirements.yaml")
    skinny_requirements = read_requirements_yaml(requirements_dir / "skinny-requirements.yaml")
    _check_skinny_tracing_mismatch(
        skinny_reqs=skinny_requirements, tracing_reqs=tracing_requirements
    )
    core_requirements = read_requirements_yaml(requirements_dir / "core-requirements.yaml")
    gateways_requirements = read_requirements_yaml(requirements_dir / "gateway-requirements.yaml")
    genai_requirements = read_requirements_yaml(requirements_dir / "genai-requirements.yaml")
    package_version = re.search(
        r'^VERSION = "([a-z0-9\.]+)"$', Path("mlflow", "version.py").read_text(), re.MULTILINE
    ).group(1)
    python_version = Path(".python-version").read_text().strip()
    versions_yaml = read_package_versions_yml()
    langchain_requirements = [
        "langchain>={},<={}".format(
            max(
                Version(versions_yaml["langchain"]["autologging"]["minimum"]),
                Version(versions_yaml["langchain"]["models"]["minimum"]),
            ),
            min(
                Version(versions_yaml["langchain"]["autologging"]["maximum"]),
                Version(versions_yaml["langchain"]["models"]["maximum"]),
            ),
        )
    ]

    match package_type:
        case PackageType.TRACING:
            dependencies = sorted(tracing_requirements)
        case PackageType.SKINNY:
            dependencies = sorted(skinny_requirements)
        case PackageType.RELEASE:
            dependencies = [
                f"mlflow-skinny=={package_version}",
                f"mlflow-tracing=={package_version}",
            ] + sorted(core_requirements)
        case PackageType.DEV:
            # skinny_requirements is an exact superset of tracing_requirements
            # (validated above), so we don't need to include both below.
            dependencies = sorted(core_requirements + skinny_requirements)
        case _:
            raise ValueError(f"Unreachable: {package_type}")

    if dep_duplicates := find_duplicates(dependencies):
        raise RuntimeError(f"Duplicated dependencies are found: {dep_duplicates}")

    match package_type:
        case PackageType.TRACING:
            package_name = "mlflow-tracing"
        case PackageType.SKINNY:
            package_name = "mlflow-skinny"
        case _:
            package_name = "mlflow"

    description = (
        "MLflow is an open source platform for the complete machine learning lifecycle"
        if package_type != PackageType.TRACING
        else (
            "MLflow Tracing SDK is an open-source, lightweight Python package that only "
            "includes the minimum set of dependencies and functionality to instrument "
            "your code/models/agents with MLflow Tracing."
        )
    )

    data = {
        "build-system": {
            "requires": ["setuptools"],
            "build-backend": "setuptools.build_meta",
        },
        "project": {
            "name": package_name,
            "version": package_version,
            "maintainers": [
                {"name": "Databricks", "email": "mlflow-oss-maintainers@googlegroups.com"}
            ],
            "description": description,
            "readme": "README_SKINNY.md" if package_type == PackageType.SKINNY else "README.md",
            "license": {
                "file": "LICENSE.txt",
            },
            "keywords": ["mlflow", "ai", "databricks"],
            "classifiers": [
                "Development Status :: 5 - Production/Stable",
                "Intended Audience :: Developers",
                "Intended Audience :: End Users/Desktop",
                "Intended Audience :: Science/Research",
                "Intended Audience :: Information Technology",
                "Topic :: Scientific/Engineering :: Artificial Intelligence",
                "Topic :: Software Development :: Libraries :: Python Modules",
                "License :: OSI Approved :: Apache Software License",
                "Operating System :: OS Independent",
                f"Programming Language :: Python :: {python_version}",
            ],
            "requires-python": f">={python_version}",
            "dependencies": dependencies,
            "optional-dependencies": {
                "extras": [
                    # Required to log artifacts and models to HDFS artifact locations
                    "pyarrow",
                    # Required to sign outgoing request with SigV4 signature
                    "requests-auth-aws-sigv4",
                    # Required to log artifacts and models to AWS S3 artifact locations
                    "boto3",
                    "botocore",
                    # Required to log artifacts and models to GCS artifact locations
                    "google-cloud-storage>=1.30.0",
                    "azureml-core>=1.2.0",
                    # Required to log artifacts to SFTP artifact locations
                    "pysftp",
                    # Required by the mlflow.projects module, when running projects against
                    # a remote Kubernetes cluster
                    "kubernetes",
                    "virtualenv",
                    # Required for exporting metrics from the MLflow server to Prometheus
                    # as part of the MLflow server monitoring add-on
                    "prometheus-flask-exporter",
                ],
                "databricks": [
                    # Required to write model artifacts to unity catalog locations
                    "azure-storage-file-datalake>12",
                    "google-cloud-storage>=1.30.0",
                    "boto3>1",
                    "botocore",
                    "databricks-agents>=1.2.0,<2.0",
                ],
                "mlserver": [
                    # Required to serve models through MLServer
                    "mlserver>=1.2.0,!=1.3.1,<2.0.0",
                    "mlserver-mlflow>=1.2.0,!=1.3.1,<2.0.0",
                ],
                "gateway": gateways_requirements,
                "genai": genai_requirements,
                # click 8.3.0 causes MLflow MCP server to fail: https://github.com/mlflow/mlflow/issues/18747
                "mcp": ["fastmcp<3,>=2.0.0", "click!=8.3.0"],
                "sqlserver": ["mlflow-dbstore"],
                "aliyun-oss": ["aliyunstoreplugin"],
                "jfrog": ["mlflow-jfrog-plugin"],
                "langchain": langchain_requirements,
                "auth": ["Flask-WTF<2"],
            }
            # Tracing SDK does not support extras
            if package_type != PackageType.TRACING
            else None,
            "urls": {
                "homepage": "https://mlflow.org",
                "issues": "https://github.com/mlflow/mlflow/issues",
                "documentation": "https://mlflow.org/docs/latest",
                "repository": "https://github.com/mlflow/mlflow",
            },
            "scripts": {
                "mlflow": "mlflow.cli:cli",
            }
            if package_type != PackageType.TRACING
            else None,
            "entry-points": {
                "mlflow.app": {
                    "basic-auth": "mlflow.server.auth:create_app",
                },
                "mlflow.app.client": {
                    "basic-auth": "mlflow.server.auth.client:AuthServiceClient",
                },
                "mlflow.deployments": {
                    "databricks": "mlflow.deployments.databricks",
                    "http": "mlflow.deployments.mlflow",
                    "https": "mlflow.deployments.mlflow",
                    "openai": "mlflow.deployments.openai",
                },
            }
            if package_type != PackageType.TRACING
            else None,
        },
        "tool": {
            "setuptools": {
                "packages": {
                    "find": {
                        "where": ["."],
                        "include": ["mlflow", "mlflow.*"]
                        if package_type != PackageType.TRACING
                        else TRACING_INCLUDE_FILES,
                        "exclude": ["tests", "tests.*"]
                        if package_type != PackageType.TRACING
                        else TRACING_EXCLUDE_FILES,
                        "namespaces": False,
                    }
                },
                "package-data": _get_package_data(package_type),
            }
        },
    }

    if package_type == PackageType.TRACING:
        out_path = Path("libs/tracing/pyproject.toml")
        write_toml_file_if_changed(out_path, package_type.description(), data)
    elif package_type == PackageType.SKINNY:
        out_path = Path("libs/skinny/pyproject.toml")
        write_toml_file_if_changed(out_path, package_type.description(), data)

        skinny_readme_path = Path("libs/skinny/README_SKINNY.md")
        new_readme_content = SKINNY_README.lstrip() + Path("README.md").read_text()
        write_file_if_changed(skinny_readme_path, new_readme_content)

        for f in ["LICENSE.txt", "MANIFEST.in", "mlflow"]:
            symlink = Path("libs/skinny", f)
            if symlink.exists():
                symlink.unlink()
            target = Path("../..", f)
            symlink.symlink_to(target, target_is_directory=target.is_dir())
    elif package_type == PackageType.RELEASE:
        out_path = Path(f"pyproject.{package_type.value}.toml")
        write_toml_file_if_changed(out_path, package_type.description(), data)
    else:
        out_path = Path("pyproject.toml")
        original_manual_content = out_path.read_text().split(SEPARATOR)[1]
        generated_part = package_type.description() + "\n" + toml.dumps(data)
        formatted_generated_part = format_content_with_taplo(generated_part)
        formatted_full_content = formatted_generated_part + SEPARATOR + original_manual_content

        write_file_if_changed(out_path, formatted_full_content)
        subprocess.check_call(["uv", "lock"])


def _get_package_data(package_type: PackageType) -> dict[str, list[str]] | None:
    if package_type == PackageType.TRACING:
        return None

    package_data = {
        "mlflow": [
            "store/db_migrations/alembic.ini",
            "temporary_db_migrations_for_pre_1_users/alembic.ini",
            "pyspark/ml/log_model_allowlist.txt",
            "server/auth/basic_auth.ini",
            "server/auth/db/migrations/alembic.ini",
            "models/notebook_resources/**/*",
            "ai_commands/**/*.md",
        ]
    }

    if package_type != PackageType.SKINNY:
        package_data["mlflow"] += ["models/container/**/*", "server/js/build/**/*"]

    return package_data


def _check_skinny_tracing_mismatch(*, skinny_reqs: list[str], tracing_reqs: list[str]) -> None:
    """
    Check if the tracing requirements are a subset of the skinny requirements.
    NB: We don't make mlflow-tracing as a hard dependency of mlflow-skinny because
    it will complicate the package management (need another .release.toml file
    that is dependent by pyproject.release.toml)
    """
    if diff := set(tracing_reqs) - set(skinny_reqs):
        raise RuntimeError(
            "Tracing requirements must be a subset of skinny requirements. "
            "Please check the requirements/skinny-requirements.yaml and "
            "requirements/tracing-requirements.yaml files.\n"
            f"Diff: {diff}"
        )


def main() -> None:
    if not Path("bin/taplo").exists():
        print(
            "taplo is required to generate pyproject.toml. "
            "Please run 'python bin/install.py' to install it."
        )
        sys.exit(1)

    for package_type in PackageType:
        build(package_type)


if __name__ == "__main__":
    main()
```

--------------------------------------------------------------------------------

---[FILE: README.md]---
Location: mlflow-master/dev/README.md

```text
## MLflow Dev Scripts

This directory contains automation scripts for MLflow developers and the build infrastructure.

## Job Statuses

[![Examples Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/examples.yml.svg?branch=master&event=schedule&label=Examples&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/examples.yml?query=workflow%3AExamples+event%3Aschedule)
[![Cross Version Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/cross-version-tests.yml.svg?branch=master&event=schedule&label=Cross%20version%20tests&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/cross-version-tests.yml?query=workflow%3A%22Cross+version+tests%22+event%3Aschedule)
[![Cross Version Test Visualization](https://img.shields.io/github/actions/workflow/status/mlflow/dev/xtest-viz.yml.svg?branch=master&event=schedule&label=Test%20Results%20Viz&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/xtest-viz.yml)
[![R-devel Action Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/r.yml.svg?branch=master&event=schedule&label=r-devel&style=for-the-badge&logo=github)](https://github.com/mlflow/dev/actions/workflows/r.yml?query=workflow%3AR+event%3Aschedule)
[![Test Requirements Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/requirements.yml.svg?branch=master&event=schedule&label=test%20requirements&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/requirements.yml?query=workflow%3A%22Test+requirements%22+event%3Aschedule)
[![Push Images Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow/push-images.yml.svg?event=release&label=push-images&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow/actions/workflows/push-images.yml?query=event%3Arelease)
[![Slow Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/dev/slow-tests.yml.svg?branch=master&event=schedule&label=slow-tests&logo=github&style=for-the-badge)](https://github.com/mlflow/dev/actions/workflows/slow-tests.yml?query=event%3Aschedule)
[![Website E2E Tests Status](https://img.shields.io/github/actions/workflow/status/mlflow/mlflow-website/e2e.yml.svg?branch=main&event=schedule&label=website-e2e&logo=github&style=for-the-badge)](https://github.com/mlflow/mlflow-website/actions/workflows/e2e.yml?query=event%3Aschedule)
```

--------------------------------------------------------------------------------

---[FILE: remove-conda-envs.sh]---
Location: mlflow-master/dev/remove-conda-envs.sh

```bash
#!/usr/bin/env bash

set -ex

mlflow_envs=$(
  conda env list |                 # list (env name, env path) pairs
  cut -d' ' -f1 |                  # extract env names
  grep "^mlflow-[a-z0-9]\{40\}\$"  # filter envs created by mlflow
) || true

if [ ! -z "$mlflow_envs" ]; then
  for env in $mlflow_envs
  do
    conda remove --all --yes --name $env
  done
fi

conda clean --all --yes
conda env list

set +ex
```

--------------------------------------------------------------------------------

---[FILE: remove_experimental_decorators.py]---
Location: mlflow-master/dev/remove_experimental_decorators.py

```python
"""
Script to automatically remove @experimental decorators from functions
that have been experimental for more than 6 months.
"""

import argparse
import ast
import json
import subprocess
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from urllib.request import urlopen


@dataclass
class ExperimentalDecorator:
    version: str
    line_number: int
    end_line_number: int
    column: int
    age_days: int
    content: str


def get_tracked_python_files() -> list[Path]:
    """Get all tracked Python files in the repository."""
    result = subprocess.check_output(["git", "ls-files", "*.py"], text=True)
    return [Path(f) for f in result.strip().split("\n") if f]


def get_mlflow_release_dates() -> dict[str, datetime]:
    """Fetch MLflow release dates from PyPI API."""
    with urlopen("https://pypi.org/pypi/mlflow/json") as response:
        data = json.loads(response.read().decode())

    release_dates: dict[str, datetime] = {}
    for version, releases in data["releases"].items():
        if releases:  # Some versions might have empty release lists
            # Get the earliest release date for this version
            upload_times: list[str] = [r["upload_time"] for r in releases if "upload_time" in r]
            if upload_times:
                earliest_time = min(upload_times)
                # Parse ISO format datetime and convert to UTC
                release_date = datetime.fromisoformat(earliest_time.replace("Z", "+00:00"))
                if release_date.tzinfo is None:
                    release_date = release_date.replace(tzinfo=timezone.utc)
                release_dates[version] = release_date

    return release_dates


def find_experimental_decorators(
    file_path: Path, release_dates: dict[str, datetime], now: datetime
) -> list[ExperimentalDecorator]:
    """
    Find all @experimental decorators in a Python file using AST and return their information
    with computed age.
    """
    content = file_path.read_text()
    tree = ast.parse(content)
    decorators: list[ExperimentalDecorator] = []

    for node in ast.walk(tree):
        if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
            continue

        for decorator in node.decorator_list:
            if not isinstance(decorator, ast.Call):
                continue

            if not (isinstance(decorator.func, ast.Name) and decorator.func.id == "experimental"):
                continue

            version = _extract_version_from_ast_decorator(decorator)
            if not version or version not in release_dates:
                continue

            release_date = release_dates[version]
            age_days = (now - release_date).days

            decorator_info = ExperimentalDecorator(
                version=version,
                line_number=decorator.lineno,
                end_line_number=decorator.end_lineno or decorator.lineno,
                column=decorator.col_offset + 1,  # 1-indexed
                age_days=age_days,
                content=ast.unparse(decorator),
            )
            decorators.append(decorator_info)

    return decorators


def _extract_version_from_ast_decorator(decorator: ast.Call) -> str | None:
    """Extract version string from AST decorator node."""
    for keyword in decorator.keywords:
        if keyword.arg == "version" and isinstance(keyword.value, ast.Constant):
            return str(keyword.value.value)
    return None


def remove_decorators_from_file(
    file_path: Path,
    decorators_to_remove: list[ExperimentalDecorator],
    dry_run: bool,
) -> list[ExperimentalDecorator]:
    if not decorators_to_remove:
        return []

    lines = file_path.read_text().splitlines(keepends=True)
    # Create a set of line numbers to remove for quick lookup (handle ranges)
    lines_to_remove: set[int] = set()
    for decorator in decorators_to_remove:
        lines_to_remove.update(range(decorator.line_number, decorator.end_line_number + 1))

    new_lines: list[str] = []

    for line_num, line in enumerate(lines, 1):
        if line_num not in lines_to_remove:
            new_lines.append(line)

    if not dry_run:
        file_path.write_text("".join(new_lines))

    return decorators_to_remove


def main() -> None:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Remove @experimental decorators older than 6 months"
    )
    parser.add_argument(
        "--dry-run", action="store_true", help="Show what would be removed without making changes"
    )
    parser.add_argument(
        "files", nargs="*", help="Python files to process (defaults to all tracked Python files)"
    )

    args = parser.parse_args()
    release_dates = get_mlflow_release_dates()
    # Calculate cutoff date (6 months ago from now)
    now = datetime.now(timezone.utc)
    cutoff_date = now - timedelta(days=6 * 30)  # Approximate 6 months
    print(f"Cutoff date: {cutoff_date.strftime('%Y-%m-%d %H:%M:%S UTC')}")

    python_files = [Path(f) for f in args.files] if args.files else get_tracked_python_files()
    for file_path in python_files:
        if not file_path.exists():
            continue

        # First, find all experimental decorators in the file with computed ages
        decorators = find_experimental_decorators(file_path, release_dates, now)
        if not decorators:
            continue

        # Filter to only decorators that should be removed (older than 6 months)
        old_decorators = [d for d in decorators if d.age_days > 6 * 30]  # 6 months approx
        if not old_decorators:
            continue

        # Remove old decorators
        if removed := remove_decorators_from_file(file_path, old_decorators, args.dry_run):
            for decorator in removed:
                action = "Would remove" if args.dry_run else "Removed"
                print(
                    f"{file_path}:{decorator.line_number}:{decorator.column}: "
                    f"{action} {decorator.content} (age: {decorator.age_days} days)"
                )


if __name__ == "__main__":
    main()
```

--------------------------------------------------------------------------------

---[FILE: requirements.txt]---
Location: mlflow-master/dev/requirements.txt

```text
# Dev script dependencies
click
ruamel.yaml.clib!=0.2.7
ruamel.yaml
requests
packaging
pydantic
pyyaml
toml
```

--------------------------------------------------------------------------------

---[FILE: ruff.py]---
Location: mlflow-master/dev/ruff.py

```python
import os
import re
import subprocess
import sys

RUFF = [sys.executable, "-m", "ruff", "check"]
MESSAGE_REGEX = re.compile(r"^.+:\d+:\d+: ([A-Z0-9]+) (\[\*\] )?.+$")


def transform(stdout: str, is_maintainer: bool) -> str:
    transformed = []
    for line in stdout.splitlines():
        if m := MESSAGE_REGEX.match(line):
            if m.group(2) is not None:
                command = (
                    "`ruff --fix .` or comment `/autoformat`" if is_maintainer else "`ruff --fix .`"
                )
                line = f"{line}. Run {command} to fix this error."
            else:
                line = (
                    f"{line}. See https://docs.astral.sh/ruff/rules/{m.group(1)} for how to fix "
                    f"this error."
                )
        transformed.append(line)
    return "\n".join(transformed)


def main():
    if "NO_FIX" in os.environ:
        with subprocess.Popen(
            [
                *RUFF,
                *sys.argv[1:],
            ],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        ) as prc:
            stdout, stderr = prc.communicate()
            is_maintainer = os.environ.get("IS_MAINTAINER", "false").lower() == "true"
            sys.stdout.write(transform(stdout, is_maintainer))
            sys.stderr.write(stderr)
            sys.exit(prc.returncode)
    else:
        with subprocess.Popen(
            [
                *RUFF,
                "--fix",
                "--exit-non-zero-on-fix",
                *sys.argv[1:],
            ]
        ) as prc:
            prc.communicate()
            sys.exit(prc.returncode)


if __name__ == "__main__":
    main()
```

--------------------------------------------------------------------------------

---[FILE: run-dev-server.sh]---
Location: mlflow-master/dev/run-dev-server.sh

```bash
#!/bin/bash
set -e

# Helper function to kill processes matching a pattern
# Usage: kill_matching_processes <pattern> <description>
function kill_matching_processes {
  local pattern="$1"
  local description="$2"
  
  if pgrep -f "$pattern" > /dev/null; then
    echo "Stopping existing $description..."
    pkill -f "$pattern" || true
    sleep 1
  fi
}

# Clean up any existing MLflow and yarn dev servers
# Using specific patterns to avoid killing unrelated processes
echo "Checking for existing dev servers..."
kill_matching_processes "mlflow.*server.*--dev" "MLflow dev server"
kill_matching_processes "mlflow/server/js.*yarn.*start" "yarn dev server"

# Parse command line arguments
env_file=""
while [[ $# -gt 0 ]]; do
  case $1 in
    --env-file)
      env_file="$2"
      shift 2
      ;;
    *)
      echo "Unknown option: $1"
      echo "Usage: $0 [--env-file <path>]"
      exit 1
      ;;
  esac
done

function wait_server_ready {
  for backoff in 0 1 2 4 8; do
    echo "Waiting for tracking server to be ready..."
    sleep $backoff
    if curl --fail --silent --show-error --output /dev/null $1; then
      echo "Server is ready"
      return 0
    fi
  done
  echo -e "\nFailed to launch tracking server"
  return 1
}

mkdir -p outputs
echo 'Running tracking server in the background'

# Handle backend store URI (tracking store)
if [ -n "$MLFLOW_TRACKING_URI" ]; then
  backend_store_uri="--backend-store-uri $MLFLOW_TRACKING_URI"
  default_artifact_root="--default-artifact-root mlruns"
elif [ -n "$MLFLOW_BACKEND_STORE_URI" ]; then
  backend_store_uri="--backend-store-uri $MLFLOW_BACKEND_STORE_URI"
  default_artifact_root="--default-artifact-root mlruns"
else
  backend_store_uri=""
  default_artifact_root=""
fi

# Handle registry store URI (model registry)
if [ -n "$MLFLOW_REGISTRY_URI" ]; then
  registry_store_uri="--registry-store-uri $MLFLOW_REGISTRY_URI"
else
  registry_store_uri=""
fi

# Build env file option
if [ -n "$env_file" ]; then
  env_file_opt="--env-file $env_file"
  echo "Using environment file: $env_file"
else
  env_file_opt=""
fi

if [ ! -d "mlflow/server/js/node_modules" ]; then
  pushd mlflow/server/js
  yarn install
  popd
fi

# Pass env-file option to mlflow command (before 'server' subcommand)
mlflow $env_file_opt server $backend_store_uri $default_artifact_root $registry_store_uri --dev &
wait_server_ready localhost:5000/health
(cd mlflow/server/js && yarn start)
```

--------------------------------------------------------------------------------

---[FILE: run-python-skinny-tests.sh]---
Location: mlflow-master/dev/run-python-skinny-tests.sh

```bash
#!/usr/bin/env bash

# Executes a subset of mlflow tests that is supported with fewer dependencies than the core mlflow package.
# Tests include most client interactions and compatibility points with the mlflow plugins around tracking, projects, models, deployments, and the cli.

# The SQL alchemy store's dependencies are added for a base client/store that can be tested against.
# A different example client/store with a minimal dependency footprint could also work for this purpose.

set -x
# Set err=1 if any commands exit with non-zero status as described in
# https://stackoverflow.com/a/42219754
err=0
trap 'err=1' ERR
export MLFLOW_SKINNY='true'

pytest tests/test_skinny_client_omits_sql_libs.py

# After verifying skinny client does not include store specific requirements,
# we are installing sqlalchemy store requirements as our example store for the test suite.
# SQL Alchemy serves as a simple, fully featured option to test skinny client store scenarios.
python -m pip install sqlalchemy alembic cryptography

# Given the example store does not delete dependencies, we verify non store related dependencies
# after the example store setup. This verifies both the example store and skinny client do not add
# unintended libraries.
pytest tests/test_skinny_client_omits_data_science_libs.py

# Install numpy that is required by mlflow.types.schema and pre-installed in DBR.
python -m pip install numpy

pytest \
  tests/test_runs.py \
  tests/tracking/test_client.py \
  tests/tracking/test_tracking.py \
  tests/projects/test_projects.py \
  tests/deployments/test_cli.py \
  tests/deployments/test_deployments.py \
  tests/projects/test_projects_cli.py \
  tests/utils/test_requirements_utils.py::test_infer_requirements_excludes_mlflow \
  tests/utils/test_search_utils.py \
  tests/store/tracking/test_file_store.py \
  tests/utils/test_doctor.py \
  --import-mode=importlib

python -m pip install pandas
pytest tests/test_skinny_client_autolog_without_scipy.py

test $err = 0
```

--------------------------------------------------------------------------------

---[FILE: server.py]---
Location: mlflow-master/dev/server.py

```python
"""
Runs MLflow server, gateway, and UI in development mode.
"""

import os
import socket
import subprocess
import sys
import time


def random_port() -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(("", 0))
        return s.getsockname()[1]


def main():
    gateway_port = random_port()
    gateway_host = "localhost"
    with (
        subprocess.Popen(
            [
                sys.executable,
                "-m",
                "mlflow",
                "gateway",
                "start",
                "--config-path",
                "examples/gateway/openai/config.yaml",
                "--host",
                gateway_host,
                "--port",
                str(gateway_port),
            ]
        ) as gateway,
        subprocess.Popen(
            [
                sys.executable,
                "-m",
                "mlflow",
                "server",
                "--dev",
            ],
            env={
                **os.environ,
                "MLFLOW_DEPLOYMENTS_TARGET": f"http://{gateway_host}:{gateway_port}",
            },
        ) as server,
        subprocess.Popen(
            [
                "yarn",
                "start",
            ],
            cwd="mlflow/server/js",
        ) as ui,
    ):
        while True:
            try:
                time.sleep(1)
            except KeyboardInterrupt:
                gateway.terminate()
                server.terminate()
                ui.terminate()
                break


if __name__:
    main()
```

--------------------------------------------------------------------------------

---[FILE: setup-ssh.sh]---
Location: mlflow-master/dev/setup-ssh.sh

```bash
# Establish SSH to localhost for test_sftp_artifact_repo.py
ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
ssh-keyscan -H localhost >> ~/.ssh/known_hosts
ssh $(whoami)@localhost exit
export LOGNAME=$(whoami)
```

--------------------------------------------------------------------------------

````
