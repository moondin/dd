---
source_txt: fullstack_samples/mlflow-master
converted_utc: 2025-12-18T11:25:53Z
part: 229
parts_total: 991
---

# FULLSTACK CODE DATABASE SAMPLES mlflow-master

## Verbatim Content (Part 229 of 991)

````text
================================================================================
FULLSTACK SAMPLES CODE DATABASE (VERBATIM) - mlflow-master
================================================================================
Generated: December 18, 2025
Source: fullstack_samples/mlflow-master
================================================================================

NOTES:
- This output is verbatim because the source is user-owned.
- Large/binary files may be skipped by size/binary detection limits.

================================================================================

---[FILE: ml_package_versions.py]---
Location: mlflow-master/mlflow/ml_package_versions.py
Signals: Pydantic

```python
# This file was auto-generated by update_ml_package_versions.py.
# Please do not edit it manually.

_ML_PACKAGE_VERSIONS = {
    "semantic_kernel": {
        "package_info": {
            "pip_release": "semantic-kernel"
        },
        "autologging": {
            "minimum": "1.34.0",
            "maximum": "1.39.0"
        }
    },
    "openai": {
        "package_info": {
            "pip_release": "openai"
        },
        "models": {
            "minimum": "1.57.3",
            "maximum": "2.9.0"
        },
        "autologging": {
            "minimum": "1.57.3",
            "maximum": "2.9.0"
        }
    },
    "dspy": {
        "package_info": {
            "pip_release": "dspy"
        },
        "models": {
            "minimum": "2.5.43",
            "maximum": "3.0.4"
        },
        "autologging": {
            "minimum": "2.5.43",
            "maximum": "3.0.4"
        }
    },
    "langchain": {
        "package_info": {
            "pip_release": "langchain"
        },
        "models": {
            "minimum": "0.3.12",
            "maximum": "1.1.3"
        },
        "autologging": {
            "minimum": "0.3.12",
            "maximum": "1.1.3"
        }
    },
    "langgraph": {
        "package_info": {
            "pip_release": "langgraph"
        },
        "models": {
            "minimum": "0.2.60",
            "maximum": "1.0.4"
        },
        "autologging": {
            "minimum": "0.2.60",
            "maximum": "1.0.4"
        }
    },
    "llama_index": {
        "package_info": {
            "pip_release": "llama-index",
            "module_name": "llama_index.core"
        },
        "models": {
            "minimum": "0.12.6",
            "maximum": "0.14.10"
        },
        "autologging": {
            "minimum": "0.12.6",
            "maximum": "0.14.10"
        }
    },
    "ag2": {
        "package_info": {
            "pip_release": "ag2",
            "module_name": "autogen"
        },
        "autologging": {
            "minimum": "0.7.0",
            "maximum": "0.10.2"
        }
    },
    "autogen": {
        "package_info": {
            "pip_release": "autogen-agentchat",
            "module_name": "autogen_agentchat"
        },
        "autologging": {
            "minimum": "0.4.9",
            "maximum": "0.7.5"
        }
    },
    "gemini": {
        "package_info": {
            "pip_release": "google-genai",
            "module_name": "google.genai"
        },
        "autologging": {
            "minimum": "1.0.0",
            "maximum": "1.55.0"
        }
    },
    "anthropic": {
        "package_info": {
            "pip_release": "anthropic"
        },
        "autologging": {
            "minimum": "0.41.0",
            "maximum": "0.75.0"
        }
    },
    "crewai": {
        "package_info": {
            "pip_release": "crewai",
            "module_name": "crewai"
        },
        "autologging": {
            "minimum": "0.95.0",
            "maximum": "1.7.0"
        }
    },
    "agno": {
        "package_info": {
            "pip_release": "agno",
            "module_name": "agno"
        },
        "autologging": {
            "minimum": "1.7.0",
            "maximum": "2.3.10"
        }
    },
    "pydantic_ai": {
        "package_info": {
            "pip_release": "pydantic-ai",
            "module_name": "pydantic_ai"
        },
        "autologging": {
            "minimum": "0.1.9",
            "maximum": "1.30.0"
        }
    },
    "smolagents": {
        "package_info": {
            "pip_release": "smolagents",
            "module_name": "smolagents"
        },
        "autologging": {
            "minimum": "1.14.0",
            "maximum": "1.23.0"
        }
    },
    "strands": {
        "package_info": {
            "pip_release": "strands-agents",
            "module_name": "strands"
        },
        "autologging": {
            "minimum": "1.4.0",
            "maximum": "1.19.0"
        }
    },
    "mistral": {
        "package_info": {
            "pip_release": "mistralai",
            "module_name": "mistralai"
        },
        "autologging": {
            "minimum": "1.2.6",
            "maximum": "1.9.11"
        }
    },
    "litellm": {
        "package_info": {
            "pip_release": "litellm"
        },
        "autologging": {
            "minimum": "1.63.14",
            "maximum": "1.74.9"
        }
    },
    "groq": {
        "package_info": {
            "pip_release": "groq"
        },
        "autologging": {
            "minimum": "0.13.1",
            "maximum": "0.37.1"
        }
    },
    "bedrock": {
        "package_info": {
            "pip_release": "boto3",
            "module_name": "boto3"
        },
        "autologging": {
            "minimum": "1.35.80",
            "maximum": "1.42.7"
        }
    },
    "sklearn": {
        "package_info": {
            "pip_release": "scikit-learn"
        },
        "models": {
            "minimum": "1.4.0",
            "maximum": "1.8.0"
        },
        "autologging": {
            "minimum": "1.4.0",
            "maximum": "1.8.0"
        }
    },
    "pytorch": {
        "package_info": {
            "pip_release": "torch",
            "module_name": "torch"
        },
        "models": {
            "minimum": "2.1.2",
            "maximum": "2.9.1"
        },
        "autologging": {
            "minimum": "2.1.2",
            "maximum": "2.9.1"
        }
    },
    "pytorch-lightning": {
        "package_info": {
            "pip_release": "pytorch-lightning",
            "module_name": "lightning"
        },
        "models": {
            "minimum": "2.1.3",
            "maximum": "2.6.0"
        },
        "autologging": {
            "minimum": "2.1.3",
            "maximum": "2.6.0"
        }
    },
    "keras": {
        "package_info": {
            "pip_release": "keras"
        },
        "models": {
            "minimum": "3.0.2",
            "maximum": "3.12.0"
        },
        "autologging": {
            "minimum": "3.0.2",
            "maximum": "3.12.0"
        }
    },
    "tensorflow": {
        "package_info": {
            "pip_release": "tensorflow"
        },
        "models": {
            "minimum": "2.16.0",
            "maximum": "2.20.0"
        },
        "autologging": {
            "minimum": "2.16.0",
            "maximum": "2.20.0"
        }
    },
    "xgboost": {
        "package_info": {
            "pip_release": "xgboost"
        },
        "models": {
            "minimum": "2.0.3",
            "maximum": "3.1.2"
        },
        "autologging": {
            "minimum": "2.0.3",
            "maximum": "3.1.2"
        }
    },
    "lightgbm": {
        "package_info": {
            "pip_release": "lightgbm"
        },
        "models": {
            "minimum": "4.2.0",
            "maximum": "4.6.0"
        },
        "autologging": {
            "minimum": "4.2.0",
            "maximum": "4.6.0"
        }
    },
    "catboost": {
        "package_info": {
            "pip_release": "catboost"
        },
        "models": {
            "minimum": "1.2.3",
            "maximum": "1.2.8"
        }
    },
    "onnx": {
        "package_info": {
            "pip_release": "onnx"
        },
        "models": {
            "minimum": "1.17.0",
            "maximum": "1.20.0"
        }
    },
    "spacy": {
        "package_info": {
            "pip_release": "spacy"
        },
        "models": {
            "minimum": "3.7.4",
            "maximum": "3.8.11"
        }
    },
    "statsmodels": {
        "package_info": {
            "pip_release": "statsmodels"
        },
        "models": {
            "minimum": "0.14.1",
            "maximum": "0.14.6"
        },
        "autologging": {
            "minimum": "0.14.1",
            "maximum": "0.14.6"
        }
    },
    "spark": {
        "package_info": {
            "pip_release": "pyspark",
            "module_name": "pyspark"
        },
        "models": {
            "minimum": "3.2.1",
            "maximum": "4.0.1"
        },
        "autologging": {
            "minimum": "3.3.0",
            "maximum": "4.0.1"
        }
    },
    "prophet": {
        "package_info": {
            "pip_release": "prophet"
        },
        "models": {
            "minimum": "1.1.6",
            "maximum": "1.2.1"
        }
    },
    "pmdarima": {
        "package_info": {
            "pip_release": "pmdarima"
        },
        "models": {
            "minimum": "2.1.0",
            "maximum": "2.1.1"
        }
    },
    "h2o": {
        "package_info": {
            "pip_release": "h2o"
        },
        "models": {
            "minimum": "3.44.0.3",
            "maximum": "3.46.0.9"
        }
    },
    "shap": {
        "package_info": {
            "pip_release": "shap"
        },
        "models": {
            "minimum": "0.44.1",
            "maximum": "0.50.0"
        }
    },
    "paddle": {
        "package_info": {
            "pip_release": "paddlepaddle"
        },
        "models": {
            "minimum": "2.6.2",
            "maximum": "3.2.2"
        },
        "autologging": {
            "minimum": "2.6.2",
            "maximum": "3.2.2"
        }
    },
    "transformers": {
        "package_info": {
            "pip_release": "transformers"
        },
        "models": {
            "minimum": "4.38.2",
            "maximum": "4.57.3"
        },
        "autologging": {
            "minimum": "4.38.2",
            "maximum": "4.57.3"
        }
    },
    "haystack": {
        "package_info": {
            "pip_release": "haystack-ai",
            "module_name": "haystack"
        },
        "autologging": {
            "minimum": "2.0.0",
            "maximum": "2.21.0"
        }
    },
    "sentence_transformers": {
        "package_info": {
            "pip_release": "sentence-transformers"
        },
        "models": {
            "minimum": "2.3.1",
            "maximum": "5.1.2"
        }
    },
    "johnsnowlabs": {
        "package_info": {
            "pip_release": "johnsnowlabs"
        },
        "models": {
            "minimum": "5.2.0",
            "maximum": "6.2.0"
        }
    }
}

# A mapping of flavor name to the module name to be imported for autologging.
# This is used for checking version compatibility in autologging.
# DO NOT EDIT MANUALLY

# GenAI packages
GENAI_FLAVOR_TO_MODULE_NAME = {
    "semantic_kernel": "semantic_kernel",
    "openai": "openai",
    "dspy": "dspy",
    "langchain": "langchain",
    "llama_index": "llama_index.core",
    "ag2": "autogen",
    "autogen": "autogen_agentchat",
    "gemini": "google.genai",
    "anthropic": "anthropic",
    "crewai": "crewai",
    "agno": "agno",
    "pydantic_ai": "pydantic_ai",
    "smolagents": "smolagents",
    "strands": "strands",
    "mistral": "mistralai",
    "litellm": "litellm",
    "groq": "groq",
    "bedrock": "boto3"
}

# Non-GenAI packages
NON_GENAI_FLAVOR_TO_MODULE_NAME = {
    "sklearn": "sklearn",
    "pytorch": "torch",
    "pytorch-lightning": "lightning",
    "keras": "keras",
    "tensorflow": "tensorflow",
    "xgboost": "xgboost",
    "lightgbm": "lightgbm",
    "statsmodels": "statsmodels",
    "spark": "pyspark",
    "paddle": "paddle",
    "transformers": "transformers",
    "haystack": "haystack",
    "pyspark.ml": "pyspark"
}

# Combined mapping for backward compatibility
FLAVOR_TO_MODULE_NAME = NON_GENAI_FLAVOR_TO_MODULE_NAME | GENAI_FLAVOR_TO_MODULE_NAME
```

--------------------------------------------------------------------------------

---[FILE: runs.py]---
Location: mlflow-master/mlflow/runs.py

```python
"""
CLI for runs
"""

import json

import click

import mlflow
from mlflow import MlflowClient
from mlflow.entities import RunStatus, ViewType
from mlflow.environment_variables import MLFLOW_EXPERIMENT_ID, MLFLOW_EXPERIMENT_NAME
from mlflow.exceptions import MlflowException
from mlflow.tracking import _get_store
from mlflow.utils.string_utils import _create_table
from mlflow.utils.time import conv_longdate_to_str

RUN_ID = click.option("--run-id", type=click.STRING, required=True)


@click.group("runs")
def commands():
    """
    Manage runs. To manage runs of experiments associated with a tracking server, set the
    MLFLOW_TRACKING_URI environment variable to the URL of the desired server.
    """


@commands.command("list")
@click.option(
    "--experiment-id",
    envvar=MLFLOW_EXPERIMENT_ID.name,
    type=click.STRING,
    help="Specify the experiment ID for list of runs.",
    required=True,
)
@click.option(
    "--view",
    "-v",
    default="active_only",
    help="Select view type for list experiments. Valid view types are "
    "'active_only' (default), 'deleted_only', and 'all'.",
)
def list_run(experiment_id: str, view: str) -> None:
    """
    List all runs of the specified experiment in the configured tracking server.
    """
    store = _get_store()
    view_type = ViewType.from_string(view) if view else ViewType.ACTIVE_ONLY
    runs = store.search_runs([experiment_id], None, view_type)
    table = []
    for run in runs:
        run_name = run.info.run_name or ""
        table.append([conv_longdate_to_str(run.info.start_time), run_name, run.info.run_id])
    click.echo(_create_table(sorted(table, reverse=True), headers=["Date", "Name", "ID"]))


@commands.command("delete")
@RUN_ID
def delete_run(run_id: str) -> None:
    """
    Mark a run for deletion. Return an error if the run does not exist or
    is already marked. You can restore a marked run with ``restore_run``,
    or permanently delete a run in the backend store.
    """
    store = _get_store()
    store.delete_run(run_id)
    click.echo(f"Run with ID {run_id} has been deleted.")


@commands.command("restore")
@RUN_ID
def restore_run(run_id: str) -> None:
    """
    Restore a deleted run.
    Returns an error if the run is active or has been permanently deleted.
    """
    store = _get_store()
    store.restore_run(run_id)
    click.echo(f"Run with id {run_id} has been restored.")


@commands.command("describe")
@RUN_ID
def describe_run(run_id: str) -> None:
    """
    All of run details will print to the stdout as JSON format.
    """
    store = _get_store()
    run = store.get_run(run_id)
    json_run = json.dumps(run.to_dictionary(), indent=4)
    click.echo(json_run)


@commands.command("create")
@click.option(
    "--experiment-id",
    envvar=MLFLOW_EXPERIMENT_ID.name,
    type=click.STRING,
    help="ID of the experiment under which to create the run. "
    "Must specify either this or --experiment-name.",
)
@click.option(
    "--experiment-name",
    envvar=MLFLOW_EXPERIMENT_NAME.name,
    type=click.STRING,
    help="Name of the experiment under which to create the run. "
    "Must specify either this or --experiment-id.",
)
@click.option(
    "--run-name",
    type=click.STRING,
    help="Optional human-readable name for the run (e.g., 'baseline-model-v1').",
)
@click.option(
    "--description",
    type=click.STRING,
    help="Optional longer description of what this run represents.",
)
@click.option(
    "--tags",
    "-t",
    multiple=True,
    help="Key-value pairs to categorize and filter runs. Use multiple times for "
    "multiple tags. Format: key=value (e.g., env=prod, model=xgboost, version=1.0).",
)
@click.option(
    "--status",
    type=click.Choice(["FINISHED", "FAILED", "KILLED"], case_sensitive=False),
    default="FINISHED",
    help="Final status of the run. Options: FINISHED (default), FAILED, or KILLED.",
)
@click.option(
    "--parent-run-id",
    type=click.STRING,
    help="Optional ID of a parent run to create a nested run under.",
)
def create_run(
    experiment_id: str | None,
    experiment_name: str | None,
    run_name: str | None,
    description: str | None,
    tags: tuple[str, ...],
    status: str,
    parent_run_id: str | None,
) -> None:
    """
    Create a new MLflow run and immediately end it with the specified status.

    This command is useful for creating runs programmatically for testing, scripting,
    or recording completed experiments. The run will be created and immediately closed
    with the specified status (FINISHED, FAILED, or KILLED).
    """
    # Validate that exactly one of experiment_id or experiment_name is provided
    if (experiment_id is not None and experiment_name is not None) or (
        experiment_id is None and experiment_name is None
    ):
        raise click.UsageError("Must specify exactly one of --experiment-id or --experiment-name.")

    # Parse tags from key=value format
    tags_dict = {}
    if tags:
        for tag in tags:
            match tag.split("=", 1):
                case [key, value]:
                    if key in tags_dict:
                        raise click.UsageError(f"Duplicate tag key: '{key}'")
                    tags_dict[key] = value
                case _:
                    raise click.UsageError(
                        f"Invalid tag format: '{tag}'. Tags must be in key=value format."
                    )

    # Set the experiment if using experiment_name
    if experiment_name:
        experiment = mlflow.set_experiment(experiment_name=experiment_name)
        experiment_id = experiment.experiment_id

    # Start the run with the specified parameters
    try:
        # Start the run
        active_run = mlflow.start_run(
            experiment_id=experiment_id,
            run_name=run_name,
            nested=bool(parent_run_id),
            parent_run_id=parent_run_id,
            tags=tags_dict,
            description=description,
        )
        run_id = active_run.info.run_id
        actual_experiment_id = active_run.info.experiment_id

        # End the run with the specified status
        mlflow.end_run(status=RunStatus.to_string(getattr(RunStatus, status.upper())))

        # Output the created run information
        output = {
            "run_id": run_id,
            "experiment_id": actual_experiment_id,
            "status": status.upper(),
            "run_name": run_name,
        }

        click.echo(json.dumps(output, indent=2))

    except MlflowException as e:
        raise click.ClickException(f"Failed to create run: {e.message}")
    except Exception as e:
        raise click.ClickException(f"Unexpected error creating run: {e!s}")


@commands.command("link-traces")
@click.option(
    "--run-id",
    type=click.STRING,
    required=True,
    help="ID of the run to link traces to.",
)
@click.option(
    "trace_ids",
    "--trace-id",
    "-t",
    multiple=True,
    required=True,
    help="Trace ID to link to the run. Can be specified multiple times (maximum 100 traces).",
)
def link_traces(run_id: str, trace_ids: tuple[str, ...]) -> None:
    """
    Link traces to a run.

    This command links one or more traces to an existing run. Traces can be
    linked to runs to establish relationships between traces and runs.
    Maximum 100 traces can be linked in a single command.
    """
    try:
        client = MlflowClient()
        client.link_traces_to_run(list(trace_ids), run_id)

        # Output success message with count
        click.echo(f"Successfully linked {len(trace_ids)} trace(s) to run '{run_id}'")

    except MlflowException as e:
        raise click.ClickException(f"Failed to link traces: {e.message}")
    except Exception as e:
        raise click.ClickException(f"Unexpected error linking traces: {e!s}")
```

--------------------------------------------------------------------------------

---[FILE: version.py]---
Location: mlflow-master/mlflow/version.py

```python
# Copyright 2018 Databricks, Inc.
import importlib.metadata
import re

VERSION = "3.8.1.dev0"


def is_release_version():
    return bool(re.match(r"^\d+\.\d+\.\d+$", VERSION))


def _is_package_installed(package_name: str) -> bool:
    try:
        importlib.metadata.version(package_name)
        return True
    except importlib.metadata.PackageNotFoundError:
        return False


# A flag to indicate whether the environment only has the tracing SDK
# installed, or includes the full MLflow or mlflow-skinny package.
# This is used to determine whether to import modules that require
# dependencies that are not included in the tracing SDK.
IS_TRACING_SDK_ONLY = not any(_is_package_installed(pkg) for pkg in ["mlflow", "mlflow-skinny"])

# A flag to indicate whether the environment only has the mlflow-skinny package
IS_MLFLOW_SKINNY = _is_package_installed("mlflow-skinny") and not _is_package_installed("mlflow")

IS_FULL_MLFLOW = _is_package_installed("mlflow")
```

--------------------------------------------------------------------------------

---[FILE: __init__.py]---
Location: mlflow-master/mlflow/__init__.py
Signals: Pydantic

```python
"""
The ``mlflow`` module provides a high-level "fluent" API for starting and managing MLflow runs.
For example:

.. code:: python

    import mlflow

    mlflow.start_run()
    mlflow.log_param("my", "param")
    mlflow.log_metric("score", 100)
    mlflow.end_run()

You can also use the context manager syntax like this:

.. code:: python

    with mlflow.start_run() as run:
        mlflow.log_param("my", "param")
        mlflow.log_metric("score", 100)

which automatically terminates the run at the end of the ``with`` block.

The fluent tracking API is not currently threadsafe. Any concurrent callers to the tracking API must
implement mutual exclusion manually.

For a lower level API, see the :py:mod:`mlflow.client` module.
"""

import contextlib
from typing import TYPE_CHECKING

from mlflow.version import IS_TRACING_SDK_ONLY, VERSION

__version__ = VERSION

import mlflow.mismatch

# `check_version_mismatch` must be called here before importing any other modules
with contextlib.suppress(Exception):
    mlflow.mismatch._check_version_mismatch()

if not IS_TRACING_SDK_ONLY:
    from mlflow import (
        artifacts,  # noqa: F401
        client,  # noqa: F401
        config,  # noqa: F401
        data,  # noqa: F401
        exceptions,  # noqa: F401
        genai,  # noqa: F401
        models,  # noqa: F401
        projects,  # noqa: F401
        tracking,  # noqa: F401
    )

from mlflow import tracing  # noqa: F401
from mlflow.environment_variables import MLFLOW_CONFIGURE_LOGGING
from mlflow.exceptions import MlflowException
from mlflow.utils.lazy_load import LazyLoader
from mlflow.utils.logging_utils import _configure_mlflow_loggers

# Lazily load mlflow flavors to avoid excessive dependencies.
anthropic = LazyLoader("mlflow.anthropic", globals(), "mlflow.anthropic")
ag2 = LazyLoader("mlflow.ag2", globals(), "mlflow.ag2")
agno = LazyLoader("mlflow.agno", globals(), "mlflow.agno")
autogen = LazyLoader("mlflow.autogen", globals(), "mlflow.autogen")
bedrock = LazyLoader("mlflow.bedrock", globals(), "mlflow.bedrock")
catboost = LazyLoader("mlflow.catboost", globals(), "mlflow.catboost")
crewai = LazyLoader("mlflow.crewai", globals(), "mlflow.crewai")
dspy = LazyLoader("mlflow.dspy", globals(), "mlflow.dspy")
gemini = LazyLoader("mlflow.gemini", globals(), "mlflow.gemini")
groq = LazyLoader("mlflow.groq", globals(), "mlflow.groq")
h2o = LazyLoader("mlflow.h2o", globals(), "mlflow.h2o")
haystack = LazyLoader("mlflow.haystack", globals(), "mlflow.haystack")
johnsnowlabs = LazyLoader("mlflow.johnsnowlabs", globals(), "mlflow.johnsnowlabs")
keras = LazyLoader("mlflow.keras", globals(), "mlflow.keras")
langchain = LazyLoader("mlflow.langchain", globals(), "mlflow.langchain")
lightgbm = LazyLoader("mlflow.lightgbm", globals(), "mlflow.lightgbm")
litellm = LazyLoader("mlflow.litellm", globals(), "mlflow.litellm")
llama_index = LazyLoader("mlflow.llama_index", globals(), "mlflow.llama_index")
llm = LazyLoader("mlflow.llm", globals(), "mlflow.llm")
metrics = LazyLoader("mlflow.metrics", globals(), "mlflow.metrics")
mistral = LazyLoader("mlflow.mistral", globals(), "mlflow.mistral")
onnx = LazyLoader("mlflow.onnx", globals(), "mlflow.onnx")
openai = LazyLoader("mlflow.openai", globals(), "mlflow.openai")
paddle = LazyLoader("mlflow.paddle", globals(), "mlflow.paddle")
pmdarima = LazyLoader("mlflow.pmdarima", globals(), "mlflow.pmdarima")
prophet = LazyLoader("mlflow.prophet", globals(), "mlflow.prophet")
pydantic_ai = LazyLoader("mlflow.pydantic_ai", globals(), "mlflow.pydantic_ai")
pyfunc = LazyLoader("mlflow.pyfunc", globals(), "mlflow.pyfunc")
pyspark = LazyLoader("mlflow.pyspark", globals(), "mlflow.pyspark")
pytorch = LazyLoader("mlflow.pytorch", globals(), "mlflow.pytorch")
rfunc = LazyLoader("mlflow.rfunc", globals(), "mlflow.rfunc")
semantic_kernel = LazyLoader("mlflow.semantic_kernel", globals(), "mlflow.semantic_kernel")
sentence_transformers = LazyLoader(
    "mlflow.sentence_transformers",
    globals(),
    "mlflow.sentence_transformers",
)
shap = LazyLoader("mlflow.shap", globals(), "mlflow.shap")
sklearn = LazyLoader("mlflow.sklearn", globals(), "mlflow.sklearn")
smolagents = LazyLoader("mlflow.smolagents", globals(), "mlflow.smolagents")
spacy = LazyLoader("mlflow.spacy", globals(), "mlflow.spacy")
strands = LazyLoader("mlflow.strands", globals(), "mlflow.strands")
spark = LazyLoader("mlflow.spark", globals(), "mlflow.spark")
statsmodels = LazyLoader("mlflow.statsmodels", globals(), "mlflow.statsmodels")
tensorflow = LazyLoader("mlflow.tensorflow", globals(), "mlflow.tensorflow")
# TxtAI integration is defined at https://github.com/neuml/mlflow-txtai
txtai = LazyLoader("mlflow.txtai", globals(), "mlflow_txtai")
transformers = LazyLoader("mlflow.transformers", globals(), "mlflow.transformers")
xgboost = LazyLoader("mlflow.xgboost", globals(), "mlflow.xgboost")

if TYPE_CHECKING:
    # Do not move this block above the lazy-loaded modules above.
    # All the lazy-loaded modules above must be imported here for code completion to work in IDEs.
    from mlflow import (  # noqa: F401
        ag2,
        agno,
        anthropic,
        autogen,
        bedrock,
        catboost,
        crewai,
        dspy,
        gemini,
        groq,
        h2o,
        haystack,
        johnsnowlabs,
        keras,
        langchain,
        lightgbm,
        litellm,
        llama_index,
        llm,
        metrics,
        mistral,
        onnx,
        openai,
        paddle,
        pmdarima,
        prophet,
        pydantic_ai,
        pyfunc,
        pyspark,
        pytorch,
        rfunc,
        semantic_kernel,
        sentence_transformers,
        shap,
        sklearn,
        smolagents,
        spacy,
        spark,
        statsmodels,
        strands,
        tensorflow,
        transformers,
        xgboost,
    )

if MLFLOW_CONFIGURE_LOGGING.get() is True:
    _configure_mlflow_loggers(root_module_name=__name__)

# Core modules required for mlflow-tracing
from mlflow.tracing.assessment import (
    delete_assessment,
    get_assessment,
    log_assessment,
    log_expectation,
    log_feedback,
    override_feedback,
    update_assessment,
)
from mlflow.tracing.fluent import (
    add_trace,
    delete_trace_tag,
    get_active_trace_id,
    get_current_active_span,
    get_last_active_trace_id,
    get_trace,
    log_trace,
    search_traces,
    set_trace_tag,
    start_span,
    start_span_no_context,
    trace,
    update_current_trace,
)
from mlflow.tracking import (
    get_tracking_uri,
    is_tracking_uri_set,
    set_tracking_uri,
)
from mlflow.tracking.fluent import active_run, flush_trace_async_logging, set_experiment

# These are minimal set of APIs to be exposed via `mlflow-tracing` package.
# APIs listed here must not depend on dependencies that are not part of `mlflow-tracing` package.
__all__ = [
    "MlflowException",
    # Minimal tracking APIs required for tracing core functionality
    "set_experiment",
    "set_tracking_uri",
    "get_tracking_uri",
    "is_tracking_uri_set",
    # NB: Tracing SDK doesn't support using Runs, however, active_run is used heavily within
    # the autologging code base.
    "active_run",
    # Tracing APIs
    "add_trace",
    "delete_trace_tag",
    "flush_trace_async_logging",
    "get_active_trace_id",
    "get_current_active_span",
    "get_last_active_trace_id",
    "get_trace",
    "log_trace",
    "search_traces",
    "set_trace_tag",
    "start_span",
    "start_span_no_context",
    "trace",
    "update_current_trace",
    # Assessment APIs
    "get_assessment",
    "delete_assessment",
    "log_assessment",
    "update_assessment",
    "log_expectation",
    "log_feedback",
    "override_feedback",
]

# Only import these modules when mlflow or mlflow-skinny is installed i.e. not importing them
# when only mlflow-tracing is installed.
if not IS_TRACING_SDK_ONLY:
    from mlflow.client import MlflowClient

    # For backward compatibility, we expose the following functions and classes at the top level in
    # addition to `mlflow.config`.
    from mlflow.config import (
        disable_system_metrics_logging,
        enable_system_metrics_logging,
        get_registry_uri,
        set_registry_uri,
        set_system_metrics_node_id,
        set_system_metrics_samples_before_logging,
        set_system_metrics_sampling_interval,
    )
    from mlflow.models.evaluation.deprecated import evaluate
    from mlflow.models.evaluation.validation import validate_evaluation_results
    from mlflow.projects import run
    from mlflow.tracking._model_registry.fluent import (
        # TODO: Prompt Registry APIs are moved to the `mlflow.genai` namespace and direct
        # imports from mlflow will be deprecated in the future.
        delete_prompt_alias,
        load_prompt,
        register_model,
        register_prompt,
        search_model_versions,
        search_prompts,
        search_registered_models,
        set_model_version_tag,
        set_prompt_alias,
    )
    from mlflow.tracking.fluent import (
        ActiveModel,
        ActiveRun,
        autolog,
        clear_active_model,
        create_experiment,
        create_external_model,
        delete_experiment,
        delete_experiment_tag,
        delete_logged_model_tag,
        delete_run,
        delete_tag,
        end_run,
        finalize_logged_model,
        flush_artifact_async_logging,
        flush_async_logging,
        get_active_model_id,
        get_artifact_uri,
        get_experiment,
        get_experiment_by_name,
        get_logged_model,
        get_parent_run,
        get_run,
        initialize_logged_model,
        last_active_run,
        last_logged_model,
        load_table,
        log_artifact,
        log_artifacts,
        log_dict,
        log_figure,
        log_image,
        log_input,
        log_inputs,
        log_metric,
        log_metrics,
        log_model_params,
        log_outputs,
        log_param,
        log_params,
        log_table,
        log_text,
        search_experiments,
        search_logged_models,
        search_runs,
        set_active_model,
        set_experiment_tag,
        set_experiment_tags,
        set_logged_model_tags,
        set_tag,
        set_tags,
        start_run,
    )
    from mlflow.tracking.multimedia import Image
    from mlflow.utils.async_logging.run_operations import RunOperations  # noqa: F401
    from mlflow.utils.credentials import login
    from mlflow.utils.doctor import doctor

    __all__ += [
        "ActiveRun",
        "ActiveModel",
        "MlflowClient",
        "MlflowException",
        "autolog",
        "clear_active_model",
        "create_experiment",
        "create_external_model",
        "delete_experiment",
        "delete_run",
        "delete_tag",
        "disable_system_metrics_logging",
        "doctor",
        "enable_system_metrics_logging",
        "end_run",
        "evaluate",
        "finalize_logged_model",
        "flush_async_logging",
        "flush_artifact_async_logging",
        "get_active_model_id",
        "get_artifact_uri",
        "get_experiment",
        "get_experiment_by_name",
        "get_logged_model",
        "get_parent_run",
        "get_registry_uri",
        "get_run",
        "initialize_logged_model",
        "last_active_run",
        "last_logged_model",
        "load_table",
        "log_artifact",
        "log_artifacts",
        "log_dict",
        "log_figure",
        "log_image",
        "log_input",
        "log_inputs",
        "log_model_params",
        "log_outputs",
        "log_metric",
        "log_metrics",
        "log_param",
        "log_params",
        "log_table",
        "log_text",
        "login",
        "pyfunc",
        "register_model",
        "run",
        "search_experiments",
        "search_logged_models",
        "search_model_versions",
        "search_registered_models",
        "search_runs",
        "search_prompts",
        "set_active_model",
        "set_experiment_tag",
        "set_experiment_tags",
        "delete_experiment_tag",
        "set_model_version_tag",
        "set_registry_uri",
        "set_system_metrics_node_id",
        "set_system_metrics_samples_before_logging",
        "set_system_metrics_sampling_interval",
        "set_tag",
        "set_tags",
        "start_run",
        "validate_evaluation_results",
        "Image",
        # Prompt Registry APIs
        # TODO: Prompt Registry APIs are moved to the `mlflow.genai` namespace and direct
        # imports from mlflow will be deprecated in the future.
        "load_prompt",
        "register_prompt",
        "search_prompts",
        "set_prompt_alias",
        "delete_prompt_alias",
        "set_logged_model_tags",
        "delete_logged_model_tag",
    ]


# `mlflow.gateway` depends on optional dependencies such as pydantic, psutil, and has version
# restrictions for dependencies. Importing this module fails if they are not installed or
# if invalid versions of these required packages are installed.
with contextlib.suppress(Exception):
    from mlflow import gateway  # noqa: F401

    __all__.append("gateway")

from mlflow.telemetry import set_telemetry_client

set_telemetry_client()
```

--------------------------------------------------------------------------------

---[FILE: __main__.py]---
Location: mlflow-master/mlflow/__main__.py

```python
from mlflow.cli import cli

cli.main()
```

--------------------------------------------------------------------------------

````
