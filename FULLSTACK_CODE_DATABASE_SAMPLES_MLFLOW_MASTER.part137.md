---
source_txt: fullstack_samples/mlflow-master
converted_utc: 2025-12-18T11:25:53Z
part: 137
parts_total: 991
---

# FULLSTACK CODE DATABASE SAMPLES mlflow-master

## Verbatim Content (Part 137 of 991)

````text
================================================================================
FULLSTACK SAMPLES CODE DATABASE (VERBATIM) - mlflow-master
================================================================================
Generated: December 18, 2025
Source: fullstack_samples/mlflow-master
================================================================================

NOTES:
- This output is verbatim because the source is user-owned.
- Large/binary files may be skipped by size/binary detection limits.

================================================================================

---[FILE: export.mdx]---
Location: mlflow-master/docs/docs/genai/tracing/opentelemetry/export.mdx

```text
import { CardGroup, SmallLogoCard } from '@site/src/components/Card';

# Export MLflow Traces/Metrics via OTLP

## Set Up OTLP Exporter

Traces generated by MLflow are compatible with the [OpenTelemetry trace spec](https://opentelemetry.io/docs/specs/otel/trace/api/#span). Therefore, MLflow traces can be exported to various observability platforms that support OpenTelemetry.

By default, MLflow exports traces to the MLflow Tracking Server. To export traces to an OpenTelemetry Collector, set the `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT` environment variable **before starting any trace**. You can also enable [dual export](#dual-export) to send traces to both MLflow and an OpenTelemetry-compatible backend simultaneously.

```bash
pip install opentelemetry-exporter-otlp
```

```python
import mlflow
import os

# Set the endpoint of the OpenTelemetry Collector
os.environ["OTEL_EXPORTER_OTLP_TRACES_ENDPOINT"] = "http://localhost:4317/v1/traces"
# Optionally, set the service name to group traces
os.environ["OTEL_SERVICE_NAME"] = "your-service-name"

# Trace will be exported to the OTel collector
with mlflow.start_span(name="foo") as span:
    span.set_inputs({"a": 1})
    span.set_outputs({"b": 2})
```

## OpenTelemetry Configuration

MLflow uses the standard OTLP exporter for exporting traces to OpenTelemetry Collector instances. You can use [all of the configuration options](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/) supported by OpenTelemetry:

```bash
export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT="http://localhost:4317/v1/traces"
export OTEL_EXPORTER_OTLP_TRACES_PROTOCOL="http/protobuf"
export OTEL_EXPORTER_OTLP_TRACES_HEADERS="api_key=12345"
```

## Integrated Observability Platforms

Click on the following icons to learn more about how to set up OpenTelemetry exporter for your specific observability platform:

<CardGroup isSmall>
    <SmallLogoCard link="https://docs.datadoghq.com/opentelemetry/">
        <span>![Datadog Logo](/images/logos/datadog-logo.png)</span>
    </SmallLogoCard>
    <SmallLogoCard link="https://docs.newrelic.com/docs/opentelemetry/get-started/apm-monitoring/opentelemetry-apm-intro/#review-settings">
        <span>![NewRelic Logo](/images/logos/new-relic-logo.png)</span>
    </SmallLogoCard>
    <SmallLogoCard link="https://signoz.io/docs/instrumentation/opentelemetry-python/">
        <span>![Signoz Logo](/images/logos/signoz-logo.svg)</span>
    </SmallLogoCard>
    <SmallLogoCard link="https://docs.splunk.com/observability/en/gdi/get-data-in/get-data-in.html">
        <span>![Splunk Logo](/images/logos/splunk-logo.png)</span>
    </SmallLogoCard>
    <SmallLogoCard link="https://grafana.com/docs/grafana-cloud/send-data/otlp/send-data-otlp/">
        <span>![Grafana Logo](/images/logos/grafana-logo.png)</span>
    </SmallLogoCard>
    <SmallLogoCard link="https://www.jaegertracing.io/docs">
        <span>![Jaeger Logo](/images/logos/jaeger-logo.png)</span>
    </SmallLogoCard>
    <SmallLogoCard link="https://docs.dynatrace.com/docs/ingest-from/opentelemetry">
        <span>![Dynatrace Logo](/images/logos/dynatrace-logo.svg)</span>
    </SmallLogoCard>
    <SmallLogoCard link="https://docs.lightstep.com/docs/collector-home-page">
        <span>![ServiceNow Logo](/images/logos/servicenow-logo.avif)</span>
    </SmallLogoCard>
</CardGroup>

## Dual Export

By default, when OTLP export is configured, MLflow sends traces only to the OpenTelemetry Collector. To send traces to both MLflow Tracking Server and OpenTelemetry Collector simultaneously, set `MLFLOW_TRACE_ENABLE_OTLP_DUAL_EXPORT=true`:

```python
import mlflow
import os

# Enable dual export
os.environ["MLFLOW_TRACE_ENABLE_OTLP_DUAL_EXPORT"] = "true"
os.environ["OTEL_EXPORTER_OTLP_TRACES_ENDPOINT"] = "http://localhost:4317/v1/traces"

# Configure MLflow tracking
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("my-experiment")

# Traces will be sent to both MLflow and the OpenTelemetry Collector
with mlflow.start_span(name="foo") as span:
    span.set_inputs({"a": 1})
    span.set_outputs({"b": 2})
```

## Metrics Export

MLflow can export OpenTelemetry metrics when a metrics endpoint is configured. This allows you to monitor span durations and other trace-related metrics in compatible monitoring systems.

**Prerequisites**: The `opentelemetry-exporter-otlp` library must be installed to enable metrics export:

```bash
pip install opentelemetry-exporter-otlp
```

To enable metrics export:

**Configure OpenTelemetry metrics endpoint**:

```bash
# For OpenTelemetry Collector (gRPC endpoint)
export OTEL_EXPORTER_OTLP_METRICS_ENDPOINT="http://localhost:4317"
export OTEL_EXPORTER_OTLP_METRICS_PROTOCOL="grpc"

# OR for OpenTelemetry Collector (HTTP endpoint)
export OTEL_EXPORTER_OTLP_METRICS_ENDPOINT="http://localhost:4318/v1/metrics"
export OTEL_EXPORTER_OTLP_METRICS_PROTOCOL="http/protobuf"
```

#### Direct Prometheus Export

Prometheus can directly receive OpenTelemetry metrics exported by MLflow:

```bash
# Configure MLflow to send metrics directly to Prometheus
export OTEL_EXPORTER_OTLP_METRICS_ENDPOINT="http://localhost:9090/api/v1/otlp/v1/metrics"
export OTEL_EXPORTER_OTLP_METRICS_PROTOCOL="http/protobuf"
```

**Prometheus configuration**: Start Prometheus with `--web.enable-otlp-receiver` and `--enable-feature=otlp-deltatocumulative` flags to accept OTLP metrics directly.

#### Exported Metrics

When enabled, MLflow exports the following OpenTelemetry histogram metric:

- **`mlflow.trace.span.duration`**: A histogram measuring span execution duration in milliseconds
  - **Unit**: `ms` (milliseconds)
  - **Labels/Attributes**:
    - `root`: `"true"` for root spans, `"false"` for child spans
    - `span_type`: The type of span (e.g., "LLM", "CHAIN", "AGENT", or "unknown")
    - `span_status`: The span status ("OK", "ERROR", or "UNSET")
    - `experiment_id`: The MLflow experiment ID associated with the trace
    - `tags.*`: All trace tags (e.g., `tags.mlflow.traceName`, `tags.mlflow.evalRequestId`)
    - `metadata.*`: All trace metadata (e.g., `metadata.mlflow.sourceRun`, `metadata.mlflow.modelId`, `metadata.mlflow.trace.tokenUsage`)

This histogram allows you to analyze:

- Response time distributions across different span types
- Performance differences between root spans and child spans
- Error rates by monitoring spans with "ERROR" status
- Performance metrics grouped by MLflow experiment
- Metrics segmented by trace tags (e.g., tags.mlflow.traceName, tags.mlflow.evalRequestId)
- Performance analysis by model ID or source run (e.g., metadata.mlflow.modelId, metadata.mlflow.sourceRun)
- Service performance trends over time

#### Complete Example

```python
import mlflow
import os

# Enable metrics collection and export
os.environ["OTEL_EXPORTER_OTLP_METRICS_ENDPOINT"] = "http://localhost:4317"
os.environ["OTEL_EXPORTER_OTLP_METRICS_PROTOCOL"] = "grpc"

# Metrics will be exported to OpenTelemetry Collector
with mlflow.start_span(name="process_request", span_type="CHAIN") as span:
    span.set_inputs({"query": "What is MLflow?"})
    # Your application logic here
    span.set_outputs({"response": "MLflow is an open source platform..."})
```
```

--------------------------------------------------------------------------------

---[FILE: index.mdx]---
Location: mlflow-master/docs/docs/genai/tracing/opentelemetry/index.mdx

```text
import FeatureHighlights from '@site/src/components/FeatureHighlights';
import useBaseUrl from '@docusaurus/useBaseUrl';
import { CircleCheck, CircleArrowRight, CircleArrowLeft } from "lucide-react";

# OpenTelemetry Integration

[OpenTelemetry](https://opentelemetry.io/) is a CNCF-backed project that provides vendor-neutral observability APIs and SDKs to instrument your applications and collect telemetry data in a consistent way. MLflow Tracing is fully compatible with OpenTelemetry, making it free from vendor lock-in.

<div style={{ justifyContent: 'center', display: 'flex', margin: '2rem 0'}}>
  <img src={useBaseUrl('/images/llms/tracing/opentelemetry/hero.png')} alt="OpenTelemetry" width="90%" />
</div>

<FeatureHighlights col={1} features={[
  {
    icon: CircleArrowRight,
    title: "Ingest OpenTelemetry Traces into MLflow",
    description: "MLflow Server exposes an OTLP endpoint at /v1/traces. This endpoint allows you to collect traces from applications written in any language that supports the OpenTelemetry protocol, such as Java, Go, Rust, etc."
  },
  {
    icon: CircleArrowLeft,
    title: "Export MLflow Traces to OpenTelemetry Backends",
    description: "Traces generated by the MLflow SDK are fully compatible with the OpenTelemetry trace spec, allowing you to export traces to any observability platform that supports the OpenTelemetry protocol, such as Datadog, Grafana, Prometheus, etc."
  },
  {
    icon: CircleCheck,
    title: "Understand Semantic Conventions",
    description: "MLflow understands popular OpenTelemetry semantic conventions for GenAI, such as GenAI Semantic Conventions, OpenInference, OpenLLMetry, etc. Traces generated with these conventions are treated as first-class citizens in MLflow and can be pipelined to other MLflow features."
  },
]} />

## OpenTelemetry-native MLflow Tracing SDK

To get started with vendor-neutral tracing quickly, you can use the OpenTelemetry-native MLflow Tracing SDK. The SDK provides a convenient one-line auto-tracing experience for [popular GenAI libraries](/genai/tracing/integrations) and enhances general OpenTelemetry traces with rich AI-specific metadata such as prompts, token usage, model name, etc. See [Quickstart](/genai/tracing/quickstart) to get started with the MLflow Tracing SDK.

```python
import mlflow
from openai import OpenAI

mlflow.openai.autolog()

client = OpenAI()
response = client.responses.create(model="gpt-5", input="Hello, world!")
```

The MLflow Tracing SDK also works seamlessly with applications already instrumented with OpenTelemetry. Enhance your existing telemetry for HTTP frameworks, databases, network calls, etc., with MLflow's AI tracing capabilities.

## Ingest OpenTelemetry Traces into MLflow

MLflow Server exposes an OTLP endpoint at `/v1/traces` ([OTLP](https://opentelemetry.io/docs/specs/otlp/)). This endpoint allows you to collect traces from applications written in any language that supports the OpenTelemetry protocol, such as Java, Go, Rust, etc.

See [Collect OpenTelemetry Traces into MLflow](/genai/tracing/opentelemetry/ingest) for more details on how to collect traces into MLflow Server.

## Export MLflow Traces/Metrics via OTLP

MLflow traces and metrics can be exported to other OpenTelemetry-compatible backends such as Datadog, Grafana, Prometheus, etc., to integrate with your existing observability platform. You can also use [dual export](/genai/tracing/opentelemetry/export#dual-export) to send traces to both MLflow and an OpenTelemetry-compatible backend simultaneously.

See [Export MLflow Traces/Metrics via OTLP](/genai/tracing/opentelemetry/export) for more details.
```

--------------------------------------------------------------------------------

---[FILE: ingest.mdx]---
Location: mlflow-master/docs/docs/genai/tracing/opentelemetry/ingest.mdx

```text
# Collect OpenTelemetry Traces into MLflow

:::info

OpenTelemetry trace ingestion is supported in **MLflow 3.6.0 and above**.

:::

## OpenTelemetry endpoint (OTLP)

MLflow Server exposes an OTLP endpoint at `/v1/traces` ([OTLP](https://opentelemetry.io/docs/specs/otlp/)). This endpoint accepts traces from any native OpenTelemetry instrumentation, allowing you to trace applications written in other languages such as Java, Go, Rust, etc.

To use this endpoint, start MLflow Server with a SQL-based backend store. The following command starts MLflow Server with an SQLite backend store:

```bash
mlflow server --backend-store-uri sqlite:///mlflow.db
```

To use other types of SQL databases such as PostgreSQL, MySQL, and MSSQL, change the store URI as described in the [backend store documentation](/self-hosting/architecture/backend-store).

In your application, configure the server endpoint and set the MLflow experiment ID in the OTLP header `x-mlflow-experiment-id`.

```bash
export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:5000/v1/traces
export OTEL_EXPORTER_OTLP_TRACES_HEADERS=x-mlflow-experiment-id=123
```

:::note

As of MLflow 3.6.0, MLflow Server supports only the OTLP/HTTP endpoint. The OTLP/gRPC endpoint is not yet supported.

:::

## Basic Example

The following example shows how to collect traces from a FastAPI application using OpenTelemetry FastAPI instrumentation.

```python
import os
import uvicorn
from fastapi import FastAPI
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

# Set the endpoint and header
MLFLOW_TRACKING_URI = "http://localhost:5000"
MLFLOW_EXPERIMENT_ID = "123"

os.environ["OTEL_EXPORTER_OTLP_TRACES_ENDPOINT"] = f"{MLFLOW_TRACKING_URI}/v1/traces"
os.environ[
    "OTEL_EXPORTER_OTLP_TRACES_HEADERS"
] = f"x-mlflow-experiment-id={MLFLOW_EXPERIMENT_ID}"


app = FastAPI()
FastAPIInstrumentor.instrument_app(app)


@app.get("/")
async def root():
    return {"message": "Hello, World!"}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## Using OpenTelemetry Collector

[OpenTelemetry Collector](https://opentelemetry.io/docs/collector/) is a vendor-agnostic agent that can be used to collect, process, and export traces to various observability platforms. To configure OpenTelemetry Collector to ingest traces into MLflow, use the following configuration:

```bash
export MLFLOW_TRACKING_URI=http://localhost:5000
export MLFLOW_EXPERIMENT_ID=123
```

```yaml title="opentelemetry-collector.yaml"
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

exporters:
  otlphttp:
    endpoint: ${MLFLOW_TRACKING_URI}
    headers:
      x-mlflow-experiment-id: ${MLFLOW_EXPERIMENT_ID}

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlphttp]
```

```bash
docker run -d --name opentelemetry-collector \
  -p 4317:4317 \
  -p 4318:4318 \
  -v $(pwd)/opentelemetry-collector.yaml:/etc/otel/collector/config.yaml \
  otel/opentelemetry-collector
```

## Compression

Since MLflow 3.7.0, MLflow's OTLP/HTTP endpoint accepts compressed trace payloads. To enable compression, set the `OTEL_EXPORTER_OTLP_TRACES_COMPRESSION` environment variable to either `gzip` or `deflate`. Other encodings are not supported.

```bash
export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:5000/v1/traces
export OTEL_EXPORTER_OTLP_TRACES_HEADERS=x-mlflow-experiment-id=123
export OTEL_EXPORTER_OTLP_TRACES_COMPRESSION=gzip
```

When routing through the OpenTelemetry Collector, set compression on the OTLP HTTP exporter so it forwards a compressed request to MLflow:

```yaml
exporters:
  otlphttp:
    endpoint: ${MLFLOW_TRACKING_URI}
    headers:
      x-mlflow-experiment-id: ${MLFLOW_EXPERIMENT_ID}
    compression: gzip
```
```

--------------------------------------------------------------------------------

---[FILE: index.mdx]---
Location: mlflow-master/docs/docs/genai/tracing/quickstart/index.mdx

```text
import ImageBox from '@site/src/components/ImageBox';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import TabsWrapper from '@site/src/components/TabsWrapper';
import ServerSetup from "@site/src/content/setup_server.mdx";
import { CardGroup, TitleCard } from "@site/src/components/Card";

# Tracing Quickstart

This quickstart guide will walk you through setting up a simple GenAI application with MLflow Tracing. In less than 10 minutes, you'll enable tracing, run a basic application, and explore the generated traces in the MLflow UI.

## Prerequisites

Make sure you have started the MLflow server.
If you don't have the MLflow server running yet, just follow these simple steps to get it started.

<ServerSetup />

## Create a MLflow Experiment

The traces your GenAI application will send to the MLflow server are grouped into MLflow experiments. We recommend creating one experiment for each GenAI application.

Let's create a new MLflow experiment using the MLflow UI so that you can start sending your traces.

<ImageBox src="/images/llms/tracing/quickstart/mlflow-ui-new-experiment.png" alt="New Experiment"/>

1. Navigate to the MLflow UI in your browser at [http://localhost:5000](http://localhost:5000).
2. Click on the <div className="inline-flex rounded-sm bg-sky-700 px-2 py-1.5 text-sm font-semibold text-white">Create</div> button on the top right.
3. Enter a name for the experiment and click on "Create".

_You can leave the `Artifact Location` field blank for now. It is an advanced configuration to override where MLflow stores experiment data._

## Dependency

To connect your GenAI application to the MLflow server, you will need to install the MLflow client SDK.

<TabsWrapper>
<Tabs groupId="programming-language">
<TabItem value="python" label="Python(OpenAI)" default>
```bash
pip install --upgrade mlflow openai>=1.0.0
```
</TabItem>

<TabItem value="typescript" label="TypeScript(OpenAI)" default>
```bash
npm install mlflow-openai
```
</TabItem>
</Tabs>
</TabsWrapper>

:::info

While this guide features an example using the OpenAI SDK, the same steps apply to other LLM providers, including Anthropic, Google, Bedrock, and many others.

For a comprehensive list of LLM providers supported by MLflow, see the <ins>[LLM Integrations Overview](/genai/tracing/integrations)</ins>.

:::

## Start Tracing

Once your experiment is created, you're ready to connect to the MLflow server and begin sending traces from your GenAI application.

<TabsWrapper>
<Tabs groupId="programming-language">
<TabItem value="python" label="Python(OpenAI)" default>

```python
import mlflow
from openai import OpenAI

# Specify the tracking URI for the MLflow server.
mlflow.set_tracking_uri("http://localhost:5000")

# Specify the experiment you just created for your GenAI application.
mlflow.set_experiment("My Application")

# Enable automatic tracing for all OpenAI API calls.
mlflow.openai.autolog()

client = OpenAI()
# The trace of the following is sent to the MLflow server.
client.chat.completions.create(
    model="o4-mini",
    messages=[
        {"role": "system", "content": "You are a helpful weather assistant."},
        {"role": "user", "content": "What's the weather like in Seattle?"},
    ],
)
```

</TabItem>

<TabItem value="typescript" label="TypeScript(OpenAI)">

```typescript
import { init } from "mlflow-tracing";
import { tracedOpenAI } from "mlflow-openai";
import { OpenAI } from "openai";

init({
    trackingUri: "http://localhost:5000",
    // NOTE: specifying experiment name is not yet supported in TypeScript SDK.
    // You can copy the experiment id from the experiment details on the MLflow UI.
    experimentId: "<experiment-id>",
});

// Wrap the OpenAI client with the tracedOpenAI function to enable automatic tracing.
const client = tracedOpenAI(new OpenAI());

// The trace of the following is sent to the MLflow server.
client.chat.completions.create({
    model: "o4-mini",
    messages: [
        {"role": "system", "content": "You are a helpful weather assistant."},
        {"role": "user", "content": "What's the weather like in Seattle?"},
    ],
})
```

</TabItem>

<TabItem value="otel" label="OpenTelemetry">
MLflow Server exposes an OTLP endpoint at `/v1/traces` ([OTLP](https://opentelemetry.io/docs/specs/otlp/)). This endpoint accepts traces from any native OpenTelemetry instrumentation, allowing you to trace applications written in other languages such as Java, Go, Rust, etc.

The following example shows how to collect traces from a FastAPI application using OpenTelemetry FastAPI instrumentation.

```python
import os
import uvicorn
from fastapi import FastAPI
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

# Set the endpoint and header
MLFLOW_TRACKING_URI = "http://localhost:5000"
MLFLOW_EXPERIMENT_ID = "123"

os.environ["OTEL_EXPORTER_OTLP_TRACES_ENDPOINT"] = f"{MLFLOW_TRACKING_URI}/v1/traces"
os.environ[
    "OTEL_EXPORTER_OTLP_TRACES_HEADERS"
] = f"x-mlflow-experiment-id={MLFLOW_EXPERIMENT_ID}"

app = FastAPI()
FastAPIInstrumentor.instrument_app(app)


@app.get("/")
async def root():
    return {"message": "Hello, World!"}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

For a deeper dive into using MLflow together with OpenTelemetry, see the [OpenTelemetry guide](/genai/tracing/opentelemetry).

</TabItem>
</Tabs>
</TabsWrapper>

## View Your Traces on the MLflow UI

After running the code above, go to the MLflow UI and select the "My Application" experiment, and then select the "Traces" tab. It should show the newly created trace.

<ImageBox src="/images/llms/tracing/quickstart/single-openai-trace-list.png" alt="Single Trace" />

<ImageBox src="/images/llms/tracing/quickstart/single-openai-trace-detail.png" alt="Single Trace" />

## Next Step

Congrats on sending your first trace with MLflow! Now that you've got the basics working, here is the recommended next step to deepen your understanding of tracing:

<CardGroup cols={1}>
    <TitleCard title="Automatic and Manual Tracing →" link="/genai/tracing/app-instrumentation/automatic/">
    Explore how MLflow supports both automatic tracing and manual tracing for custom logic, plus how you can combine the two to get more insightful traces.
    </TitleCard>
</CardGroup>
```

--------------------------------------------------------------------------------

---[FILE: index.mdx]---
Location: mlflow-master/docs/docs/genai/tracing/track-environments-context/index.mdx

```text
import { APILink } from "@site/src/components/APILink";

# Track Versions & Environments

Tracking the execution environment and application version of your GenAI application allows you to debug performance and quality issues relative to the code. This metadata enables:

- **Environment-specific analysis** across `development`, `staging`, and `production`
- **Performance/quality tracking** and regression detection across app versions
- **Faster root cause analysis** when issues occur

## Standard & Custom Metadata

MLflow uses immutable metadata (key-value string pairs) to store contextual information on traces.

```python
import mlflow

trace = mlflow.get_trace(trace_id="your-trace-id")
print(trace.info.trace_metadata)
# Output: {'environment': 'production', 'app_version': '1.0.0'}
```

### Automatically Populated Tags

These standard tags are automatically captured by MLflow based on your execution environment:

| Metadata Field              | Description                                            | Automatic Setting Logic                                                                                                                                                                                                                                                                           |
| --------------------------- | ------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `mlflow.source.name`        | The entry point or script that generated the trace.    | Automatically populated with the filename for Python scripts, notebook name for Databricks/Jupyter notebooks.                                                                                                                                                                                     |
| `mlflow.source.git.commit`  | Git commit hash.                                       | If run from a Git repository, the commit hash is automatically detected and populated.                                                                                                                                                                                                            |
| `mlflow.source.git.branch`  | Git branch.                                            | If run from a Git repository, the current branch name is automatically detected and populated.                                                                                                                                                                                                    |
| `mlflow.source.git.repoURL` | Git repo URL.                                          | If run from a Git repository, the repository URL is automatically detected and populated.                                                                                                                                                                                                         |
| `mlflow.source.type`        | Captures the execution environment.                    | Automatically set to `NOTEBOOK` if running in Jupyter or Databricks notebook, `LOCAL` if running a local Python script, else `UNKNOWN` (automatically detected). <br/><br/>_In your deployed app, we suggest updating this variable based on the environment e.g., `PRODUCTION`, `STAGING`, etc._ |
| `mlflow.sourceRun`          | The run ID of the source run that generated the trace. | Automatically populated with the run ID of the source run that generated the trace.                                                                                                                                                                                                               |

:::note

There are other trace metadata fields set by MLflow, such as `mlflow.trace.sizeBytes`, `mlflow.trace.tokenUsage`, etc. They are not related to the application versioning but are useful for performance analysis.

:::

### Reserved Standard Metadata

Some standard metadata fields have special meaning but must be set manually:

| Metadata Field         | Description                                                           | Automatic Setting Logic                                                                   |
| ---------------------- | --------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| `mlflow.trace.session` | Groups traces from multi-turn conversations or user sessions together | Automatically populated with the session ID of the user session that generated the trace. |
| `mlflow.trace.user`    | Associates traces with specific users for user-centric analysis       | Automatically populated with the user ID of the user that generated the trace.            |

See the [Setting Metadata](#setting-metadata) section for more details on how to set these metadata from your application.

### Custom Metadata

You can define custom metadata or override the automatically populated metadata to capture any business-specific or application-specific context. For example, you might want to attach information such as:

- `app_version`: e.g., `"1.0.0"` (from `APP_VERSION` environment variable)
- `deployment_id`: e.g., `"deploy-abc-123"` (from `DEPLOYMENT_ID` environment variable)
- `region`: e.g., `"us-east-1"` (from `REGION` environment variable)
- _(Other custom tags like feature flags can also be added)_

## Setting Metadata

To set metadata on a trace from your application, use the <APILink fn="mlflow.update_current_trace" /> API and pass the metadata in the `metadata` parameter.

```python
mlflow.update_current_trace(metadata={"key": "value"})
```

The real application code might look like this:

```python
import mlflow
import os
from fastapi import FastAPI, Request
from pydantic import BaseModel

app = FastAPI()


class ChatRequest(BaseModel):
    message: str


@mlflow.trace  # Ensure @mlflow.trace is the outermost decorator
@app.post("/chat")  # FastAPI decorator should be inner
def handle_chat(request: Request, chat_request: ChatRequest):
    # Retrieve all context from request headers
    client_request_id = request.headers.get("X-Request-ID")
    session_id = request.headers.get("X-Session-ID")
    user_id = request.headers.get("X-User-ID")

    # Update the current trace with all context and environment metadata
    # The @mlflow.trace decorator ensures an active trace is available
    mlflow.update_current_trace(
        client_request_id=client_request_id,
        metadata={
            # Reserved session metadata - groups traces from multi-turn conversations
            "mlflow.trace.session": session_id,
            # Reserved user metadata - associates traces with specific users
            "mlflow.trace.user": user_id,
            # Override automatically populated environment metadata
            "mlflow.source.type": os.getenv(
                "APP_ENVIRONMENT", "development"
            ),  # Override default LOCAL/NOTEBOOK
            # Add custom environment metadata
            "environment": "production",
            "app_version": os.getenv("APP_VERSION", "1.0.0"),
            "deployment_id": os.getenv("DEPLOYMENT_ID", "unknown"),
            "region": os.getenv("REGION", "us-east-1"),
        },
    )

    # --- Your application logic for processing the chat message ---
    # For example, calling a language model with context
    # response_text = my_llm_call(
    #     message=chat_request.message,
    #     session_id=session_id,
    #     user_id=user_id
    # )
    response_text = f"Processed message: '{chat_request.message}'"

    return {"response": response_text}


# To run this example (requires uvicorn and fastapi):
# uvicorn your_file_name:app --reload
#
# Example curl request with context headers:
# curl -X POST "http://127.0.0.1:8000/chat" \
#      -H "Content-Type: application/json" \
#      -H "X-Request-ID: req-abc-123-xyz-789" \
#      -H "X-Session-ID: session-def-456-uvw-012" \
#      -H "X-User-ID: user-jane-doe-12345" \
#      -d '{"message": "What is my account balance?"}'
```

## Querying and Analyzing Context Data

### Using the MLflow UI

In the MLflow UI (Traces Tab) and the <APILink fn="mlflow.search_traces" /> API, you can search for traces by context metadata using the following search queries:

- `metadata.environment = 'production'`
- `metadata.app_version = '2.1.0'`
- ``metadata.`mlflow.trace.session` = 'session-abc-456'``

View the full list of supported filter syntax in the [Search Traces](/genai/tracing/search-traces) guide.
```

--------------------------------------------------------------------------------

---[FILE: index.mdx]---
Location: mlflow-master/docs/docs/genai/tracing/track-users-sessions/index.mdx

```text
import { APILink } from "@site/src/components/APILink";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import TilesGrid from "@site/src/components/TilesGrid";
import TileCard from "@site/src/components/TileCard";
import { Search, TrendingUp } from "lucide-react";
import useBaseUrl from '@docusaurus/useBaseUrl';

# Track Users & Sessions

<video src={useBaseUrl("/images/llms/tracing/chat-sessions-demo.mp4")} controls loop autoPlay muted aria-label="Traces with session IDs" />

Many real-world AI applications use session to maintain multi-turn user interactions. MLflow Tracing provides built-in support for associating traces with users and grouping them into sessions. Tracking users and sessions in your GenAI application provides essential context for understanding user behavior, analyzing conversation flows, and improving personalization.

## Store User and Session IDs in Metadata

:::note New in MLflow 3
The standard metadata for user and session tracking is only available in MLflow 3 and above. To upgrade, please run `pip install --upgrade mlflow`.
:::

MLflow provides two standard metadata fields for session and user tracking:

- `mlflow.trace.user` - Associates traces with specific users
- `mlflow.trace.session` - Groups traces belonging to multi-turn conversations

When you use these standard metadata fields, MLflow automatically enables filtering and grouping in the UI. Unlike tags, metadata cannot be updated once the trace is logged, making it ideal for immutable identifiers like user and session IDs.

## Basic Usage

To record user and session information in your application, use the <APILink fn="mlflow.update_current_trace" /> API and pass the user and session IDs in the metadata.

<Tabs>
  <TabItem value="python" label="Python" default>
    Here's how to add user and session tracking to your application:

    ```python
    import mlflow


    @mlflow.trace
    def chat_completion(message: list[dict], user_id: str, session_id: str):
        """Process a chat message with user and session tracking."""

        # Add user and session context to the current trace
        mlflow.update_current_trace(
            metadata={
                "mlflow.trace.user": user_id,  # Links trace to specific user
                "mlflow.trace.session": session_id,  # Groups trace with conversation
            }
        )

        # Your chat logic here
        return generate_response(message)
    ```

  </TabItem>
  <TabItem value="typescript" label="TypeScript">
    ```typescript
    import * as mlflow from "mlflow-tracing";

    const chatCompletion = mlflow.trace(
        (message: list[dict], user_id: str, session_id: str) => {
            // Add user and session context to the current trace
            mlflow.updateCurrentTrace({
                metadata: {
                    "mlflow.trace.user": user_id,
                    "mlflow.trace.session": session_id,
                },
            });

            // Your chat logic here
            return generate_response(message);
        },
        { name: "chat_completion" }
    );
    ```

  </TabItem>
</Tabs>

## Web Application Example

<Tabs>
  <TabItem value="python" label="Python (FastAPI)" default>

    ```python
    import mlflow
    import os
    from fastapi import FastAPI, Request
    from pydantic import BaseModel
    from openai import OpenAI

    app = FastAPI()
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    mlflow.set_tracking_uri("http://localhost:5000")
    mlflow.set_experiment(experiment_id="<your-experiment-id>")
    mlflow.openai.autolog()


    class ChatRequest(BaseModel):
        message: str


    @mlflow.trace
    def process_chat(message: str, user_id: str, session_id: str):
        # Update trace with user and session context
        mlflow.update_current_trace(
            metadata={
                "mlflow.trace.session": session_id,
                "mlflow.trace.user": user_id,
            }
        )

        # Process chat message using OpenAI API
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": message},
            ],
        )
        return response.choices[0].message.content


    @app.post("/chat")
    def handle_chat(request: Request, chat_request: ChatRequest):
        session_id = request.headers.get("X-Session-ID", "default-session")
        user_id = request.headers.get("X-User-ID", "default-user")
        response_text = process_chat(chat_request.message, user_id, session_id)
        return {"response": response_text}


    @app.get("/")
    async def root():
        return {"message": "FastAPI MLflow Tracing Example"}


    if __name__ == "__main__":
        import uvicorn

        uvicorn.run(app, host="0.0.0.0", port=8000)
    ```

  </TabItem>
  <TabItem value="typescript" label="TypeScript (express)">
    ```typescript
    import express, { Request, Response } from 'express';
    import bodyParser from 'body-parser';
    import * as mlflow from 'mlflow-tracing';
    import { tracedOpenAI } from "mlflow-openai";
    import OpenAI from 'openai';

    const app = express();
    app.use(bodyParser.json());

    const openai = tracedOpenAI(new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    }));

    mlflow.init({
      trackingUri: "http://localhost:5000",
      experimentId: "<your-experiment-id>",
    });

    class Chat {
      @mlflow.trace({ spanType: mlflow.SpanType.LLM })
      static async process(message: string, userId: string, sessionId: string) {
        // Update MLflow trace metadata for this user and session
        await mlflow.updateCurrentTrace({
          metadata: {
            'mlflow.trace.session': sessionId,
            'mlflow.trace.user': userId,
          },
        });

        const response = await openai.responses.create({
          model: 'gpt-4.1-mini',
          instructions: 'You are a helpful assistant.',
          input: message,
        });
        return response.output_text;
      }
    }

    app.post('/chat', async (req: Request, res: Response) => {
      const sessionId = req.header('X-Session-ID') || 'default-session';
      const userId = req.header('X-User-ID') || 'default-user';
      const message = req.body.message;

      try {
        const response = await Chat.process(message, userId, sessionId);
        res.json({ response: response });
      } catch (err) {
        res.status(500).json({ error: 'OpenAI request failed.' });
      }
    });

    app.get('/', (req: Request, res: Response) => {
      res.json({ message: 'Express MLflow Tracing Example' });
    });

    if (require.main === module) {
      app.listen(8000, () => {
        console.log('Server listening on http://localhost:8000');
      });
    }
    ```

  </TabItem>
</Tabs>

**Example request:**

```bash
curl -X POST http://localhost:8000/chat \
    -H "Content-Type: application/json" \
    -H "X-Session-ID: session-123" \
    -H "X-User-ID: user-456" \
    -d '{"message": "Hello, how are you?"}'
```

## Querying

<Tabs>
  <TabItem value="ui-search" label="MLflow UI Search" default>
    Filter traces in the MLflow UI using these search queries:

    ```
    # Find all traces for a specific user
    metadata.`mlflow.trace.user` = 'user-123'

    # Find all traces in a session
    metadata.`mlflow.trace.session` = 'session-abc-456'

    # Find traces for a user within a specific session
    metadata.`mlflow.trace.user` = 'user-123' AND metadata.`mlflow.trace.session` = 'session-abc-456'
    ```

  </TabItem>
  <TabItem value="user-analysis" label="Programmatic Analysis">
    Analyze user behavior patterns programmatically:

    ```python
    import mlflow
    import pandas as pd

    # Search for all traces from a specific user
    user_traces_df: pd.DataFrame = mlflow.search_traces(
        filter_string=f"metadata.`mlflow.trace.user` = '{user_id}'",
    )

    # Calculate key metrics
    total_interactions = len(user_traces_df)
    unique_sessions = user_traces_df["metadata.mlflow.trace.session"].nunique()
    avg_response_time = user_traces_df["info.execution_time_ms"].mean()
    success_rate = user_traces_df["info.state"].value_counts()["OK"] / total_interactions

    # Display the results
    print(f"User has {total_interactions} interactions across {unique_sessions} sessions")
    print(f"Average response time: {avg_response_time} ms")
    print(f"Success rate: {success_rate}")
    ```

  </TabItem>

</Tabs>

## Next Steps

<TilesGrid>
  <TileCard
    icon={Search}
    title="Search Traces"
    description="Master advanced filtering techniques for user and session analysis"
    href="/genai/tracing/search-traces"
    linkText="Learn search →"
  />
  <TileCard
    icon={TrendingUp}
    title="Production Monitoring"
    description="Set up comprehensive production observability with user context"
    href="/genai/tracing/prod-tracing"
    linkText="Monitor production →"
  />
</TilesGrid>
```

--------------------------------------------------------------------------------

````
